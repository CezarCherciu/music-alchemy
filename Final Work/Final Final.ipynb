{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats  \n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "from statsmodels.tools import add_constant\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "global object_cols\n",
    "object_cols = ['artist_name', 'track_id', 'track_name', 'key_notes','pop_cat']\n",
    "\n",
    "global numeric_cols\n",
    "numeric_cols = ['acousticness', 'danceability', 'duration_ms', 'energy', 'instrumentalness', 'key', 'liveness',\n",
    "        'loudness', 'mode', 'speechiness', 'tempo', 'time_signature', 'valence',\n",
    "       'popularity','pop_frac','pop_bin']\n",
    "\n",
    "global categorical_cols\n",
    "categorical_cols = ['key', 'mode', 'time_signature']\n",
    "\n",
    "global numeric_non_cat\n",
    "numeric_non_cat = ['acousticness', 'danceability', 'duration_ms', 'energy', 'instrumentalness', 'liveness',\n",
    "       'loudness', 'speechiness', 'tempo','valence']\n",
    "\n",
    "global cols_to_stardardize\n",
    "cols_to_standardize = ['duration_ms', 'loudness', 'tempo']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'popularity'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\boogie_woogie\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'popularity'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-745579445ee6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;31m# Calculate and plot final logistic regression values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m     \u001b[0mlogistic_regression_final\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_the_roc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m     \u001b[1;31m#plot_cutoffs_vs_metrics(df)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[1;31m#plot_conf_matrix_Train()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-745579445ee6>\u001b[0m in \u001b[0;36mlogistic_regression_final\u001b[1;34m(df, plot_the_roc)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[0mcutoff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_X_y_logistic_more_cols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_sample_combine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-745579445ee6>\u001b[0m in \u001b[0;36msplit_sample_combine\u001b[1;34m(df, cutoff, col, rand)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msplit_sample_combine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m55\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'popularity'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m     \u001b[1;31m# split out popular rows above the popularity cutoff\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m     \u001b[0msplit_pop_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[1;31m# get the leftover rows, the 'unpopular' songs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\boogie_woogie\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2799\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\boogie_woogie\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'popularity'"
     ]
    }
   ],
   "source": [
    "### ALL FUNCTIONS DEFINED HERE ####\n",
    "# load in main database of songs and attributes\n",
    "def load_data():\n",
    "    df = pd.read_csv('Datasets/dataset-of-90s.csv')\n",
    "    return df\n",
    "\n",
    "# set some display options so easier to view all columns at once\n",
    "def set_view_options(max_cols=50, max_rows=50, max_colwidth=9, dis_width=250):\n",
    "    pd.options.display.max_columns = max_cols\n",
    "    pd.options.display.max_rows = max_rows\n",
    "    pd.set_option('max_colwidth', max_colwidth)\n",
    "    pd.options.display.width = dis_width\n",
    "\n",
    "# allows for easier visualization of all columns at once in the terminal\n",
    "def rename_columns(df):\n",
    "    df.columns = ['artist', 'trk_id', 'trk_name', 'acous', 'dance', 'ms', \n",
    "                  'energy', 'instr', 'key', 'live', 'loud', 'mode', 'speech', \n",
    "                  'tempo', 't_sig', 'val']\n",
    "    return df\n",
    "\n",
    "def get_df_info(df):\n",
    "    # take an initial look at our data\n",
    "    print(df.head())\n",
    "\n",
    "    # take a look at the columns in our data set\n",
    "    print(\"The columns are:\")\n",
    "    print(df.columns)\n",
    "\n",
    "    # look at data types for each\n",
    "    print(df.info())\n",
    "\n",
    "    # take a look at data types, and it looks like we have a pretty clean data set!\n",
    "    # However, I think the 0 popularity scores might throw the model(s) off a bit.\n",
    "    print(\"Do we have any nulls?\")\n",
    "    print(f\"Looks like we have {df.isnull().sum().sum()} nulls\")\n",
    "\n",
    "    # Lets take a look at the average popularity score\n",
    "    pop_mean = df['popularity'].mean()\n",
    "    print(pop_mean)\n",
    "\n",
    "    # Proportion of songs that are very popular\n",
    "    print(df[df['popularity'] >= 50 ]['popularity'].count() / df.shape[0])\n",
    "\n",
    "    # Unique artists and song counts by artist\n",
    "    print(df['artist_name'].unique().shape)\n",
    "    print(df['artist_name'].value_counts())\n",
    "\n",
    "# nice way to truncate the column names to display easier\n",
    "# can be used with various metrics\n",
    "def describe_cols(df, L=10):\n",
    "    '''Limit ENTIRE column width (including header)'''\n",
    "    # get the max col width\n",
    "    O = pd.get_option(\"display.max_colwidth\")\n",
    "    # set max col width to be L\n",
    "    pd.set_option(\"display.max_colwidth\", L)\n",
    "    print(df.rename(columns=lambda x: x[:L - 2] + '...' if len(x) > L else x).describe())\n",
    "    pd.set_option(\"display.max_colwidth\", O)\n",
    "\n",
    "# How many songs have a popularity score > 90??\n",
    "# Let's list these songs\n",
    "def most_popular_songs(df):\n",
    "    most_popular = df[df['popularity'] > 90]['popularity'].count()\n",
    "    print(df[df['popularity'] > 90][['artist_name', 'popularity']])\n",
    "\n",
    "# plot a scatter plot\n",
    "def scatter_plot(df, col_x, col_y):\n",
    "    plt.scatter(df[col_x], df[col_y], alpha=0.2)\n",
    "    plt.show()\n",
    "\n",
    "def plot_scatter_matrix(df, num_rows):\n",
    "    scatter_matrix(df[:num_rows], alpha=0.2, figsize=(6, 6), diagonal='kde')\n",
    "    plt.show()\n",
    "\n",
    "def calc_correlations(df, cutoff):\n",
    "    corr = df.corr()\n",
    "    print(corr[corr > cutoff])\n",
    "\n",
    "# get redundant pairs from DataFrame\n",
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal pairs of correlation matrix and all pairs we'll remove \n",
    "    (since pair each is doubled in corr matrix)'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            if df[cols[i]].dtype != 'object' and df[cols[j]].dtype != 'object':\n",
    "                # print(\"THIS IS NOT AN OBJECT, YO, so you CAN take a corr of it, smarty!\")\n",
    "                pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "# get top absolute correlations\n",
    "def get_top_abs_correlations(df, n=10):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    \n",
    "    print(\"The top absolute correlations are:\")\n",
    "    print(au_corr[0:n])\n",
    "    return au_corr[0:n]\n",
    "\n",
    "# initial linear regression function, and plots\n",
    "def linear_regression_initial(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    X_cols = ['acousticness', 'danceability', 'duration_ms', 'energy', \n",
    "          'instrumentalness', 'key', 'liveness', 'loudness', 'mode', \n",
    "          'speechiness', 'tempo', 'time_signature', 'valence']\n",
    "\n",
    "    y_col = ['popularity']\n",
    "\n",
    "    X = df[X_cols]\n",
    "    y = df[y_col]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "    X_train = sm.add_constant(X_train)\n",
    "\n",
    "    # Instantiate OLS model, fit, predict, get errors\n",
    "    model = sm.OLS(y_train, X_train)\n",
    "    results = model.fit()\n",
    "    fitted_vals = results.predict(X_train)\n",
    "    stu_resid = results.resid_pearson\n",
    "    residuals = results.resid\n",
    "    y_vals = pd.DataFrame({'residuals':residuals, 'fitted_vals':fitted_vals, \\\n",
    "                           'stu_resid': stu_resid})\n",
    "\n",
    "    # Print the results\n",
    "    print(results.summary())\n",
    "\n",
    "    # QQ Plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    plt.title(\"QQ Plot - Initial Linear Regression\")\n",
    "    fig = sm.qqplot(stu_resid, line='45', fit=True, ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "    # Residuals Plot\n",
    "    y_vals.plot(kind='scatter', x='fitted_vals', y='stu_resid')\n",
    "    plt.show()\n",
    "\n",
    "# print count of all zeros within the dataset\n",
    "def get_zeros(df):\n",
    "    print(df[df['popularity'] == 0 ]['popularity'].count())\n",
    "\n",
    "# plot polularity scores distribution\n",
    "def plot_pop_dist(df):\n",
    "    # set palette\n",
    "    sns.set_palette('muted')\n",
    "\n",
    "    # create initial figure\n",
    "    fig = plt.figure(figsize=(8,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    sns.distplot(df['popularity']/100, color='g', label=\"Popularity\").set_title(\"Distribution of Popularity Scores - Entire Data Set\")\n",
    "\n",
    "    # create x and y axis labels\n",
    "    plt.xlabel(\"Popularity\")\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# plot undersampling methodology\n",
    "def undersample_plot(df):\n",
    "    # set palette\n",
    "    sns.set_palette('muted')\n",
    "\n",
    "    # create initial figure\n",
    "    fig = plt.figure(figsize=(8,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    sns.distplot(df['popularity']/100, color='g', label=\"Popularity\").set_title(\"Illustration of Undersampling from Data Set\")\n",
    "    \n",
    "    # create line to shade to the right of\n",
    "    line = ax.get_lines()[-1]\n",
    "    x_line, y_line = line.get_data()\n",
    "    mask = x_line > 0.55\n",
    "    x_line, y_line = x_line[mask], y_line[mask]\n",
    "    ax.fill_between(x_line, y1=y_line, alpha=0.5, facecolor='red')\n",
    "\n",
    "    # get values for and plot first label\n",
    "    label_x = 0.5\n",
    "    label_y = 4\n",
    "    arrow_x = 0.6\n",
    "    arrow_y = 0.2\n",
    "\n",
    "    arrow_properties = dict(\n",
    "        facecolor=\"black\", width=2,\n",
    "        headwidth=4,connectionstyle='arc3,rad=0')\n",
    "\n",
    "    plt.annotate(\n",
    "        \"First, sample all songs in this range.\\n Sample size is n. Cutoff is 0.5.\", xy=(arrow_x, arrow_y),\n",
    "        xytext=(label_x, label_y),\n",
    "        bbox=dict(boxstyle='round,pad=0.5', fc='red', alpha=0.5),\n",
    "        arrowprops=arrow_properties)\n",
    "\n",
    "    # Get values for and plot second label\n",
    "    label_x = 0.1\n",
    "    label_y = 3\n",
    "    arrow_x = 0.2\n",
    "    arrow_y = 0.2\n",
    "\n",
    "    arrow_properties = dict(\n",
    "        facecolor=\"black\", width=2,\n",
    "        headwidth=4,connectionstyle='arc3,rad=0')\n",
    "\n",
    "    plt.annotate(\n",
    "        \"Next, randomly sample \\n n songs in this range\", xy=(arrow_x, arrow_y),\n",
    "        xytext=(label_x, label_y),\n",
    "        bbox=dict(boxstyle='round,pad=0.5', fc='g', alpha=0.5),\n",
    "        arrowprops=arrow_properties)\n",
    "\n",
    "    # plot final word box\n",
    "    plt.annotate(\n",
    "        \"Therefore, end up with a 50/50 \\n split of Popular / Not Popular\\n songs\", xy=(0.6, 2),\n",
    "        xytext=(0.62, 2),\n",
    "        bbox=dict(boxstyle='round,pad=0.5', fc='b', alpha=0.5))\n",
    "\n",
    "    # create x and y axis labels\n",
    "    plt.xlabel(\"Popularity\")\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# calculate and print more stats from the df\n",
    "def get_stats(df):\n",
    "    # print stats for various metrics\n",
    "    print(f\"There are {df.shape[0]} rows\")\n",
    "    print(f\"There are {df['track_id'].unique().shape} unique songs\")\n",
    "    print(f\"There are {df['artist_name'].unique().shape} unique artists\")\n",
    "    print(f\"There are {df['popularity'].unique().shape} popularity scores\")\n",
    "    print(f\"The mean popularity score is {df['popularity'].mean()}\")\n",
    "    print(f\"There are {df[df['popularity'] > 55]['popularity'].count()} songs with a popularity score > 55\")\n",
    "    print(f\"There are {df[df['popularity'] > 75]['popularity'].count()} songs with a popularity score > 75\")\n",
    "    print(f\"Only {(df[df['popularity'] > 80]['popularity'].count() / df.shape[0])*100:.2f} % of songs have a popularity score > 80\")\n",
    "\n",
    "# plot univariate dists for several independent variables\n",
    "def plot_univ_dists(df, cutoff):\n",
    "    popularity_cutoff = cutoff\n",
    "    print('Mean value for Danceability feature for Popular songs: {}'.format(df[df['popularity'] > popularity_cutoff]['danceability'].mean()))\n",
    "    print('Mean value for Danceability feature for Unpopular songs: {}'.format(df[df['popularity'] < popularity_cutoff]['danceability'].mean()))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8,5))\n",
    "    fig.suptitle('Histograms and Univariate Distributions of Important Features')\n",
    "    sns.distplot(df[df['popularity'] < popularity_cutoff]['danceability'])\n",
    "    sns.distplot(df[df['popularity'] > popularity_cutoff]['danceability'])\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8,5))\n",
    "    sns.distplot(df[df['popularity'] < popularity_cutoff]['valence'])\n",
    "    sns.distplot(df[df['popularity'] > popularity_cutoff]['valence'])\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8,5))\n",
    "    sns.distplot(df[df['popularity'] < popularity_cutoff]['acousticness'])\n",
    "    sns.distplot(df[df['popularity'] > popularity_cutoff]['acousticness'])\n",
    "    plt.show()\n",
    "\n",
    "# plot violin plot for several independent variables\n",
    "def plot_violin(df, cutoff):\n",
    "    df = df.copy()\n",
    "    \n",
    "    sns.set(style=\"whitegrid\")\n",
    "    df['pop_bin'] = np.where(df['popularity'] > cutoff, \"Popular\", \"Not_Popular\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, sharey=True, figsize=(12,4))\n",
    "    fig.suptitle('Distributions of Selected Features at Popularity Score Cutoff of 55')\n",
    "    \n",
    "    sns.violinplot(x=df['pop_bin'], y=df['danceability'], ax=ax[0])\n",
    "    sns.violinplot(x=df['pop_bin'], y=df['valence'], ax=ax[1])\n",
    "    sns.violinplot(x=df['pop_bin'], y=df['acousticness'], ax=ax[2])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, sharey=True, figsize=(12,4))\n",
    "    fig.suptitle('Distributions of Selected Features at Popularity Score Cutoff of 55')\n",
    "\n",
    "    sns.violinplot(x=df['pop_bin'], y=df['energy'], ax=ax[0])\n",
    "    sns.violinplot(x=df['pop_bin'], y=df['instrumentalness'], ax=ax[1])\n",
    "    sns.violinplot(x=df['pop_bin'], y=df['liveness'], ax=ax[2])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# plot pairplot for subsection of df rows and columns\n",
    "def plot_pairplot(df, rows, cutoff):\n",
    "    # not it looks MUCH better to run this function in jupyter\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['pop_bin'] = np.where(df['popularity'] > cutoff, \"Popular\", \"Not_Popular\")\n",
    "    \n",
    "    cols_for_pp = ['danceability', 'energy', 'instrumentalness',\n",
    "       'loudness','valence', 'popularity', 'pop_bin']\n",
    "\n",
    "    sns.pairplot(df.loc[:rows, cols_for_pp], hue='pop_bin', size=2)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# plot the key counts for popular and unpopular songs\n",
    "def plot_keys(df, cutoff):\n",
    "    df_popular = df[df['popularity'] > cutoff].copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, sharey=True, figsize=(8,5))\n",
    "    key_mapping = {0.0: 'C', 1.0: 'C♯,D♭', 2.0: 'D', 3.0: 'D♯,E♭', 4.0: 'E', 5.0: \n",
    "                  'F', 6.0: 'F♯,G♭', 7.0: 'G', 8.0: 'G♯,A♭', 9.0: 'A', 10.0: 'A♯,B♭', \n",
    "                  11.0: 'B'}\n",
    "    \n",
    "    df_popular['key_val'] = df_popular['key'].map(key_mapping)\n",
    "    sns.countplot(x='key_val', data=df_popular, order=df_popular['key_val'].value_counts().index, palette='muted')\n",
    "    plt.title(\"Key Totals for Popular Songs\")\n",
    "    plt.show()\n",
    "\n",
    "    df_unpopular = df[df['popularity'] < 55].copy()\n",
    "    fig, ax = plt.subplots(1, 1, sharey=True, figsize=(8,5))\n",
    "    df_unpopular['key_val'] = df_unpopular['key'].map(key_mapping)\n",
    "    sns.countplot(x='key_val', data=df_unpopular, order=df_unpopular['key_val'].value_counts().index, palette='muted')\n",
    "    plt.title(\"Key Totals for Unpopular Songs\")\n",
    "    plt.show()\n",
    "\n",
    "# plot a heatmap of the correlations between features as well as dependent variable\n",
    "def plot_heatmap(df):\n",
    "    # note this looks better in jupyter as well\n",
    "    plt.figure(figsize = (16,6))\n",
    "    sns.heatmap(df.corr(), cmap=\"coolwarm\", annot=True, )\n",
    "    plt.show()\n",
    "\n",
    "# check that deltas in means are significant for selected dependent variables\n",
    "def calc_ANOVA(df, cutoff):\n",
    "    df_popular = df[df['popularity'] > cutoff].copy()\n",
    "    df_unpopular = df[df['popularity'] < cutoff].copy()\n",
    "\n",
    "    print(\"Popular and Unpopular Danceability Means:\")  \n",
    "    print(df_popular['danceability'].mean())\n",
    "    print(df_unpopular['danceability'].mean())\n",
    "    f_val, p_val = stats.f_oneway(df_popular['danceability'], df_unpopular['danceability'])  \n",
    "    \n",
    "    print(\"Danceability One-way ANOVA P ={}\".format(p_val)) \n",
    "\n",
    "    print(\"Popular and Unpopular Loudness Means:\")  \n",
    "    print(df_popular['loudness'].mean())\n",
    "    print(df_unpopular['loudness'].mean())\n",
    "    f_val, p_val = stats.f_oneway(df_popular['loudness'], df_unpopular['loudness'])  \n",
    "    \n",
    "    print(\"Loudness One-way ANOVA P ={}\".format(p_val)) \n",
    "\n",
    "    print(df_popular['valence'].mean())\n",
    "    print(df_unpopular['valence'].mean())\n",
    "    f_val, p_val = stats.f_oneway(df_popular['valence'], df_unpopular['valence'])  \n",
    "    \n",
    "    print(\"Valence One-way ANOVA P ={}\".format(p_val))\n",
    "\n",
    "    print(df_popular['instrumentalness'].mean())\n",
    "    print(df_unpopular['instrumentalness'].mean())\n",
    "    f_val, p_val = stats.f_oneway(df_popular['instrumentalness'], df_unpopular['instrumentalness'])  \n",
    "    \n",
    "    print(\"Instrumentalness One-way ANOVA P ={}\".format(p_val))\n",
    "\n",
    "# randomly sample data below cutoff after choosing a cutoff so have a 50/50 split\n",
    "# of popular/unpopular target variable values.\n",
    "def random_under_sampler(df, cutoff):\n",
    "    df_original = df.copy()\n",
    "    df_original['pop_bin'] = np.where(df_original['popularity'] > cutoff, \"Popular\", \"Not_Popular\")\n",
    "\n",
    "    df_small = df_original[df_original['popularity'] > cutoff].copy()\n",
    "    df_samples_added = df_small.copy()\n",
    "    \n",
    "    total = df_small.shape[0] + 1\n",
    "\n",
    "    # loop through and add random unpopular rows to sampled df\n",
    "    while total <= df_small.shape[0]*2:\n",
    "\n",
    "        # pick a random from from the original dataframe\n",
    "        rand_row = random.randint(0,df_original.shape[0])\n",
    "        \n",
    "        if df_original.loc[rand_row, 'pop_bin'] == \"Not_Popular\":\n",
    "            df_samples_added.loc[total] = df_original.loc[rand_row, :]\n",
    "            total +=1\n",
    "\n",
    "    # print some stats on the undersampled df\n",
    "    print(\"Size checks for new df:\")\n",
    "    print(\"Shape of new undersampled df: {}\".format(df_samples_added.shape))\n",
    "    print(df_samples_added['pop_bin'].value_counts())\n",
    "    print(df_samples_added[df_samples_added['pop_bin'] == 'Popular']['danceability'].mean())\n",
    "    print(df_samples_added[df_samples_added['pop_bin'] == 'Not_Popular']['danceability'].mean())\n",
    "    print(df_samples_added[df_samples_added['pop_bin'] == 'Popular']['danceability'].count())\n",
    "    print(df_samples_added[df_samples_added['pop_bin'] == 'Not_Popular']['danceability'].count())\n",
    "    f_val, p_val = stats.f_oneway(df_samples_added[df_samples_added['pop_bin'] == 'Popular']['danceability'], df_samples_added[df_samples_added['pop_bin'] == 'Not_Popular']['danceability'])  \n",
    "  \n",
    "    print(\"One-way ANOVA P ={}\".format(p_val))\n",
    "\n",
    "    # return the df\n",
    "    return df_samples_added\n",
    "\n",
    "# plot histograms of metrics for popular and unpopular songs\n",
    "def plot_hist(sampled_df):\n",
    "    sampled_df[sampled_df['pop_bin'] == \"Popular\"].hist(figsize=(8, 8))  \n",
    "    plt.show()\n",
    "\n",
    "    sampled_df[sampled_df['pop_bin'] != \"Popular\"].hist(figsize=(8, 8))\n",
    "    plt.show()\n",
    "\n",
    "# return records that contain strings of artist and track names\n",
    "def search_artist_track_name(df, artist, track):\n",
    "    # this displays much better in jupyter\n",
    "    print(df[(df['artist_name'].str.contains(artist)) & (df['track_name'].str.contains(track))])\n",
    "\n",
    "    # use this if searching for A$AP rocky (or other artist with $ in the name)\n",
    "    # df[(df['artist_name'].str.contains(\"A\\$AP Rocky\"))]\n",
    "\n",
    "# add important columns to dataframe\n",
    "def add_cols(df, cutoff=55):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # add key_notes mapping key num vals to notes\n",
    "    key_mapping = {0.0: 'C', 1.0: 'C♯,D♭', 2.0: 'D', 3.0: 'D♯,E♭', \n",
    "                   4.0: 'E', 5.0: 'F', 6.0: 'F♯,G♭', 7.0: 'G', \n",
    "                   8.0: 'G♯,A♭', 9.0: 'A', 10.0: 'A♯,B♭', 11.0: 'B'}\n",
    "    df['key_notes'] = df['key'].map(key_mapping)\n",
    "    \n",
    "    # add columns relating to popularity\n",
    "    df['pop_frac'] = df['popularity'] / 100\n",
    "    df['pop_cat'] = np.where(df['popularity'] > cutoff, \"Popular\", \"Not_Popular\")\n",
    "    df['pop_bin'] = np.where(df['popularity'] > cutoff, 1, 0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# choose cutoff, sample popular data, randomly sample unpopular data, and combine the dfs\n",
    "def split_sample_combine(df, cutoff=55, col='popularity', rand=None):\n",
    "    # split out popular rows above the popularity cutoff\n",
    "    split_pop_df = df[df[col] > cutoff].copy()\n",
    "    \n",
    "    # get the leftover rows, the 'unpopular' songs\n",
    "    df_leftover = df[df[col] < cutoff].copy()\n",
    "    \n",
    "    # what % of the original data do we now have?\n",
    "    ratio = split_pop_df.shape[0] / df.shape[0]\n",
    "    \n",
    "    # what % of leftover rows do we need?\n",
    "    ratio_leftover = split_pop_df.shape[0] / df_leftover.shape[0]\n",
    "    \n",
    "    # get the exact # of unpopular rows needed, using a random sampler\n",
    "    unpop_df_leftover, unpop_df_to_add = train_test_split(df_leftover, \\\n",
    "                                                          test_size=ratio_leftover, \\\n",
    "                                                          random_state = rand)\n",
    "    \n",
    "    # combine the dataframes to get total rows = split_pop_df * 2\n",
    "    # ssc stands for \"split_sample_combine\"\n",
    "    ssc_df = split_pop_df.append(unpop_df_to_add).reset_index(drop=True)\n",
    "\n",
    "    # shuffle the df\n",
    "    ssc_df = ssc_df.sample(frac=1, random_state=rand).reset_index(drop=True)\n",
    "    \n",
    "    # add key_notes mapping key num vals to notes\n",
    "    key_mapping = {0.0: 'C', 1.0: 'C♯,D♭', 2.0: 'D', 3.0: 'D♯,E♭', \n",
    "                   4.0: 'E', 5.0: 'F', 6.0: 'F♯,G♭', 7.0: 'G', \n",
    "                   8.0: 'G♯,A♭', 9.0: 'A', 10.0: 'A♯,B♭', 11.0: 'B'}\n",
    "    ssc_df['key_notes'] = ssc_df['key'].map(key_mapping)\n",
    "    \n",
    "    # add columns relating to popularity\n",
    "    ssc_df['pop_frac'] = ssc_df['popularity'] / 100\n",
    "    ssc_df['pop_cat'] = np.where(ssc_df['popularity'] > cutoff, \"Popular\", \"Not_Popular\")\n",
    "    ssc_df['pop_bin'] = np.where(ssc_df['popularity'] > cutoff, 1, 0)\n",
    "    \n",
    "    return ssc_df\n",
    "\n",
    "# standardize data and return X and y dfs for linear regresssion\n",
    "def standardize_return_X_y(df, std=True, log=False):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # standardize some columns if std = True\n",
    "    if std == True:\n",
    "        for col in cols_to_standardize:\n",
    "            new_col_name = col + \"_std\"\n",
    "            df[new_col_name] = (df[col] - df[col].mean()) / df[col].std()\n",
    "\n",
    "        X_cols = ['acousticness', 'danceability', 'duration_ms_std', 'energy', \n",
    "                  'instrumentalness', 'key', 'liveness', 'loudness_std', 'mode', \n",
    "                  'speechiness', 'tempo_std', 'time_signature', 'valence']\n",
    "    else:\n",
    "        X_cols = ['acousticness', 'danceability', 'duration_ms', 'energy', \n",
    "                  'instrumentalness', 'key', 'liveness', 'loudness', 'mode', \n",
    "                  'speechiness', 'tempo', 'time_signature', 'valence']\n",
    "        \n",
    "    # if log = True, let's transform y to LOG\n",
    "    if log == True:\n",
    "        df['pop_log'] = df['popularity'] / 100\n",
    "        df['pop_log'] = [0.00000001 if x == 0 else x for x in df['pop_log']]\n",
    "        df['pop_log'] = [0.99999999 if x == 1 else x for x in df['pop_log']]\n",
    "        df['pop_log'] = np.log(df['pop_log'] / (1 - df['pop_log']))\n",
    "        y_col = ['pop_log']\n",
    "            \n",
    "    else:\n",
    "        y_col = ['popularity']\n",
    "\n",
    "    # split into X and y\n",
    "    X = df[X_cols]\n",
    "    y = df[y_col]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# final, clean, linear regression function\n",
    "def linear_regression_final(df, show_plots=True):\n",
    "    X, y = standardize_return_X_y(df, std=True, log=False)\n",
    "\n",
    "    # Add constant\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Instantiate OLS model, fit, predict, and get errors\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "    fitted_vals = results.predict(X)\n",
    "    stu_resid = results.resid_pearson\n",
    "    residuals = results.resid\n",
    "    y_vals = pd.DataFrame({'residuals':residuals, 'fitted_vals':fitted_vals, \\\n",
    "                           'stu_resid': stu_resid})\n",
    "\n",
    "    # Maybe do a line graph for this?\n",
    "    print(results.summary())\n",
    "    \n",
    "    ### Plot predicted values vs. actual/true\n",
    "    if show_plots == True:\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        plt.title(\"True vs. Predicted Popularity Values - Initial Linear Regression\")\n",
    "        plt.plot(y,alpha=0.2, label=\"True\")\n",
    "        plt.plot(fitted_vals,alpha=0.5, c='r', label=\"Predicted\")\n",
    "        plt.ylabel(\"Popularity\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # QQ Plot\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        fig = sm.qqplot(stu_resid, line='45', fit=True, ax=ax)\n",
    "        plt.show()\n",
    "  \n",
    "\n",
    "    # Residuals Plot\n",
    "        y_vals.plot(kind='scatter', y='fitted_vals', x='stu_resid')\n",
    "        plt.show()\n",
    "\n",
    "    return results\n",
    "\n",
    "# calculate root mean squared error\n",
    "def my_rmse(y_true, y_pred):\n",
    "    mse = ((y_true - y_pred)**2).mean()\n",
    "    return np.sqrt(mse)\n",
    "\n",
    "# create a linear regression using sklearn, in order to compare models, and \n",
    "# also incorporate train_test_split into this, and calculate and print RMSE\n",
    "def linear_regression_sklearn(df, show_plots=True):\n",
    "    X, y = standardize_return_X_y(df)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "    # Fit model using the training set\n",
    "    linear = LinearRegression()\n",
    "    linear.fit(X_train, y_train)\n",
    "\n",
    "    # Call predict to get the predicted values for training and test set\n",
    "    train_predicted = linear.predict(X_train)\n",
    "    test_predicted = linear.predict(X_test)\n",
    "\n",
    "    # Calculate RMSE for training and test set\n",
    "    print('RMSE for training set {}'.format(my_rmse(y_train.values, train_predicted)))\n",
    "    print('RMSE for test set {}'.format(my_rmse(y_test.values, test_predicted)))\n",
    "    print('The Coefficients are:')\n",
    "    print(linear.coef_)\n",
    "    print('The R^2 values is: {}'.format(linear.score(X_train, y_train)))\n",
    "\n",
    "    if show_plots == True:\n",
    "        plt.plot(y_train.reset_index(drop=True), alpha=0.2)\n",
    "        plt.plot(train_predicted, alpha=0.5, c='r')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(y_test.reset_index(drop=True), alpha=0.2)\n",
    "        plt.plot(test_predicted, alpha=0.5, c='r')\n",
    "        plt.show()\n",
    "\n",
    "# various data standardization and X/y split functions for logisitic reression\n",
    "# based on the columns you want to standardize and return\n",
    "def return_X_y_logistic(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # define columns to use for each\n",
    "    X_cols = ['acousticness', 'danceability', 'duration_ms', 'energy', \n",
    "              'instrumentalness', 'key', 'liveness', 'loudness', 'mode', \n",
    "              'speechiness', 'tempo', 'time_signature', 'valence']\n",
    "\n",
    "    # use 1's and 0's for logistic\n",
    "    y_col = ['pop_bin']\n",
    "\n",
    "    # split into X and y\n",
    "    X = df[X_cols]\n",
    "    y = df[y_col]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def return_X_y_logistic_more_cols(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # define columns to use for each\n",
    "    X_cols = ['artist_name','track_id','track_name','acousticness', 'danceability', 'duration_ms', 'energy', \n",
    "              'instrumentalness', 'key', 'liveness', 'loudness', 'mode', \n",
    "              'speechiness', 'tempo', 'time_signature', 'valence']\n",
    "\n",
    "    # use 1's and 0's for logistic\n",
    "    y_col = ['pop_bin']\n",
    "\n",
    "    # split into X and y\n",
    "    X = df[X_cols]\n",
    "    y = df[y_col]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def return_X_y_logistic_sig_only(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # define columns to use for each\n",
    "    X_cols = ['danceability','energy', \n",
    "              'instrumentalness', 'loudness']\n",
    "\n",
    "    # use 1's and 0's for logistic\n",
    "    y_col = ['pop_bin']\n",
    "\n",
    "    # split into X and y\n",
    "    X = df[X_cols]\n",
    "    y = df[y_col]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def standardize_X_sig_only(X):  \n",
    "    X = X.copy()\n",
    "    \n",
    "    cols = ['loudness']\n",
    "    # standardize only columns not between 0 and 1\n",
    "    for col in cols:\n",
    "        new_col_name = col + \"_std\"\n",
    "        X[new_col_name] = (X[col] - X[col].mean()) / X[col].std()\n",
    "        \n",
    "    X_cols = ['danceability','energy', \n",
    "              'instrumentalness', 'loudness_std']\n",
    "\n",
    "    # return the std columns in a dataframe\n",
    "    X = X[X_cols]\n",
    "    \n",
    "    return X\n",
    "\n",
    "def standardize_X(X):  \n",
    "    X = X.copy()\n",
    "    \n",
    "    # standardize only columns not between 0 and 1\n",
    "    for col in cols_to_standardize:\n",
    "        new_col_name = col + \"_std\"\n",
    "        X[new_col_name] = (X[col] - X[col].mean()) / X[col].std()\n",
    "        \n",
    "    X_cols = ['acousticness', 'danceability', 'duration_ms_std', 'energy', \n",
    "                  'instrumentalness', 'key', 'liveness', 'loudness_std', 'mode', \n",
    "                  'speechiness', 'tempo_std', 'time_signature', 'valence']\n",
    "\n",
    "    # return the std columns in a dataframe\n",
    "    X = X[X_cols]\n",
    "    \n",
    "    return X\n",
    "\n",
    "def standardize_X_train_test(X_train, X_test):  \n",
    "    X_train = X_train.copy()\n",
    "    X_test = X_test.copy() \n",
    "    \n",
    "    # standardize only columns not between 0 and 1\n",
    "    for col in cols_to_standardize:\n",
    "        new_col_name = col + \"_std\"\n",
    "        X_train[new_col_name] = (X_train[col] - X_train[col].mean()) / X_train[col].std()\n",
    "        X_test[new_col_name] = (X_test[col] - X_test[col].mean()) / X_test[col].std()\n",
    "    \n",
    "    X_cols = ['acousticness', 'danceability', 'duration_ms_std', 'energy', \n",
    "                  'instrumentalness', 'key', 'liveness', 'loudness_std', 'mode', \n",
    "                  'speechiness', 'tempo_std', 'time_signature', 'valence']\n",
    "\n",
    "    # return the std columns in a dataframe\n",
    "    X_train_std = X_train[X_cols]\n",
    "    X_test_std = X_test[X_cols]\n",
    "    \n",
    "    return X_train_std, X_test_std\n",
    "\n",
    "# Create a basic logistic regression\n",
    "def basic_logistic_regression(df, cutoff=55, rand=0, sig_only=False):\n",
    "    df = df.copy()\n",
    "\n",
    "    if sig_only == True:\n",
    "        X, y = return_X_y_logistic_sig_only(split_sample_combine(df, cutoff=cutoff, rand=rand))\n",
    "        X = standardize_X_sig_only(X)\n",
    "\n",
    "    else:\n",
    "        X, y = return_X_y_logistic(split_sample_combine(df, cutoff=80, rand=rand))\n",
    "        X = standardize_X(X)\n",
    "\n",
    "    X_const = add_constant(X, prepend=True)\n",
    "\n",
    "    logit_model = Logit(y, X_const).fit()\n",
    "    \n",
    "    print(logit_model.summary())\n",
    "\n",
    "    return logit_model\n",
    "\n",
    "def logistic_regression_with_kfold(df, cutoff=55, rand=0, sig_only=False):\n",
    "    df = df.copy()\n",
    "    \n",
    "    if sig_only == True:\n",
    "        X, y = return_X_y_logistic_sig_only(split_sample_combine(df, cutoff=cutoff, rand=rand))\n",
    "        X = standardize_X_sig_only(X)\n",
    "\n",
    "    else:\n",
    "        X, y = return_X_y_logistic(split_sample_combine(df, cutoff=cutoff, rand=rand))\n",
    "        X = standardize_X(X)\n",
    "\n",
    "    X = X.values\n",
    "    y = y.values.ravel()\n",
    "\n",
    "    classifier = LogisticRegression()\n",
    "\n",
    "    # before kFold\n",
    "    y_predict = classifier.fit(X, y).predict(X)\n",
    "    y_true = y\n",
    "    accuracy_score(y_true, y_predict)\n",
    "    print(f\"accuracy: {accuracy_score(y_true, y_predict)}\")\n",
    "    print(f\"precision: {precision_score(y_true, y_predict)}\")\n",
    "    print(f\"recall: {recall_score(y_true, y_predict)}\")\n",
    "    print(f\"The coefs are: {classifier.fit(X,y).coef_}\")\n",
    "\n",
    "    # with kfold\n",
    "    kfold = KFold(len(y))\n",
    "\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for train_index, test_index in kfold:\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X[train_index], y[train_index])\n",
    "\n",
    "        y_predict = model.predict(X[test_index])\n",
    "        y_true = y[test_index]\n",
    "\n",
    "        accuracies.append(accuracy_score(y_true, y_predict))\n",
    "        precisions.append(precision_score(y_true, y_predict))\n",
    "        recalls.append(recall_score(y_true, y_predict))\n",
    "\n",
    "    print(f\"accuracy: {np.average(accuracies)}\")\n",
    "    print(f\"precision: {np.average(precisions)}\")\n",
    "    print(f\"recall: {np.average(recalls)}\")\n",
    "\n",
    "# this is the code for the final logistic regression I chose, after running all the above\n",
    "# logistic regression models and k-fold cross-val analysis\n",
    "def logistic_regression_final(df, plot_the_roc=True):\n",
    "    df = df.copy()\n",
    "    cutoff = 80\n",
    "    \n",
    "    X, y = return_X_y_logistic_more_cols(split_sample_combine(df, cutoff=cutoff, rand=2))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=2)\n",
    "\n",
    "    global df_train_results_log80 \n",
    "    global df_test_results_log80\n",
    "    df_train_results_log80 = X_train.join(y_train)\n",
    "    df_test_results_log80 = X_test.join(y_test)\n",
    "\n",
    "    # standardize X_train and X_test\n",
    "    X_train = standardize_X(X_train)\n",
    "    X_test = standardize_X(X_test)\n",
    "\n",
    "    X_train = X_train.values\n",
    "    y_train = y_train.values.ravel()\n",
    "\n",
    "    X_test = X_test.values\n",
    "    y_test = y_test.values.ravel()\n",
    "\n",
    "    global sanity_check\n",
    "    sanity_check = X_test\n",
    "\n",
    "    ## Run logistic regression on all the data\n",
    "    classifier = LogisticRegression()\n",
    "    # note using .predict_proba() below, which is the probability of each class\n",
    "    \n",
    "    #predict values for X_train\n",
    "    y_predict_train = classifier.fit(X_train,y_train).predict(X_train)\n",
    "    probs_0and1_train = classifier.fit(X_train,y_train).predict_proba(X_train)\n",
    "    y_prob_P_train = probs_0and1_train[:,1]\n",
    "\n",
    "    # predict values for X_test\n",
    "    y_predict_test = classifier.fit(X_train,y_train).predict(X_test)\n",
    "    probs_0and1_test = classifier.fit(X_train,y_train).predict_proba(X_test) # yes!\n",
    "    y_prob_P_test = probs_0and1_test[:,1]\n",
    "\n",
    "    # calculate metrics needed to use for ROC curve below\n",
    "    fpr_train, tpr_train, thresholds_train = metrics.roc_curve(y_train, y_prob_P_train, pos_label=1)\n",
    "    auc_train = metrics.roc_auc_score(y_train, y_prob_P_train) # note we are scoring on our training data!\n",
    "\n",
    "    fpr_test, tpr_test, thresholds_test = metrics.roc_curve(y_test, y_prob_P_test, pos_label=1)\n",
    "    auc_test = metrics.roc_auc_score(y_test, y_prob_P_test) # note we are scoring on our training data!\n",
    "\n",
    "    # print some metrics\n",
    "    print(\"Train accuracy: {:.2f}\".format(accuracy_score(y_train, y_predict_train)))\n",
    "    print(\"Test accuracy: {:.2f}\".format(accuracy_score(y_test, y_predict_test)))\n",
    "\n",
    "    print(\"Train recall: {:.2f}\".format(recall_score(y_train, y_predict_train)))\n",
    "    print(\"Test recall: {:.2f}\".format(recall_score(y_test, y_predict_test)))\n",
    "\n",
    "    print(\"Train precision: {:.2f}\".format(precision_score(y_train, y_predict_train)))\n",
    "    print(\"Test precision: {:.2f}\".format(precision_score(y_test, y_predict_test)))\n",
    "\n",
    "    print(\"Train auc: {:.2f}\".format(auc_train))\n",
    "    print(\"Test auc: {:.2f}\".format(auc_test))\n",
    "\n",
    "    global conf_matrix_log80_train\n",
    "    global conf_matrix_log80_test\n",
    "    conf_matrix_log80_train = confusion_matrix(y_train, y_predict_train)\n",
    "    conf_matrix_log80_test = confusion_matrix(y_test, y_predict_test)\n",
    "\n",
    "    global final_coefs\n",
    "    global final_intercept\n",
    "    final_coefs = classifier.fit(X_train,y_train).coef_\n",
    "    final_intercept = classifier.fit(X_train,y_train).intercept_\n",
    "\n",
    "    # Back of the envelope calcs to make sure metrics above are correct\n",
    "    df_train_results_log80 = df_train_results_log80.reset_index(drop=True)\n",
    "    df_train_results_log80['pop_predict'] = y_prob_P_train\n",
    "\n",
    "    df_test_results_log80 = df_test_results_log80.reset_index(drop=True)\n",
    "    df_test_results_log80['pop_predict'] = y_prob_P_test\n",
    "\n",
    "    df_train_results_log80['pop_predict_bin'] = np.where(df_train_results_log80['pop_predict'] >= 0.5, 1, 0)\n",
    "    df_test_results_log80['pop_predict_bin'] = np.where(df_test_results_log80['pop_predict'] >= 0.5, 1, 0)\n",
    "    \n",
    "    print(\"Back of the envelope calc for Train Recall\")\n",
    "    print(sum((df_train_results_log80['pop_predict_bin'].values * df_train_results_log80['pop_bin'].values))/ df_train_results_log80['pop_bin'].sum())\n",
    "\n",
    "    if plot_the_roc == True:\n",
    "        # Plot the ROC\n",
    "        fig = plt.figure(figsize=(10,8))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k',\n",
    "                label='Luck')\n",
    "        ax.plot(fpr_train, tpr_train, color='b', lw=2, label='Model_Train')\n",
    "        ax.plot(fpr_test, tpr_test, color='r', lw=2, label='Model_Test')\n",
    "        ax.set_xlabel(\"False Positive Rate\", fontsize=20)\n",
    "        ax.set_ylabel(\"True Positive Rate\", fontsize=20)\n",
    "        ax.set_title(\"ROC curve - Cutoff: \" + str(cutoff), fontsize=24)\n",
    "        ax.text(0.05, 0.95, \" \".join([\"AUC_train:\",str(auc_train.round(3))]), fontsize=20)\n",
    "        ax.text(0.32, 0.7, \" \".join([\"AUC_test:\",str(auc_test.round(3))]), fontsize=20)\n",
    "        ax.legend(fontsize=24)\n",
    "        plt.show()\n",
    "\n",
    "# print out confusion matrix\n",
    "def print_confusion_matrix(df, cutoff=55, rand=0):\n",
    "    df = df.copy()\n",
    "\n",
    "    X, y = return_X_y_logistic(split_sample_combine(df, cutoff=80, rand=rand))\n",
    "    X = standardize_X(X)\n",
    "\n",
    "    X = X.values\n",
    "    y = y.values.ravel()\n",
    "\n",
    "    ## Run logistic regression on all the data\n",
    "    classifier = LogisticRegression()\n",
    "    # note using .predict() below, which uses default 0.5 for a binary classifier\n",
    "    y_pred = classifier.fit(X,y).predict(X) # agh! this uses 0.5 threshold for binary classifier\n",
    "    y_true = y\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "    print(\"| TN | FP |\\n| FN | TP |\\n\")\n",
    "    print(cnf_matrix)\n",
    "    print(f\"The accurracy is {accuracy_score(y_true, y_pred)}\")\n",
    "    print(f\"The accurracy (check) is {(cnf_matrix[1][1]+ cnf_matrix[0][0])/np.sum(cnf_matrix)}\")\n",
    "\n",
    "# plot popularity score cutoffs vs. logistic regression metrics\n",
    "def plot_cutoffs_vs_metrics(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    df_cols = ['auc', 'accuracy', 'precision', 'recall', 'cutoff', 'type']\n",
    "    df_metrics = pd.DataFrame(columns = df_cols)\n",
    "    cutoff_range = [45, 55, 60, 65, 70, 75, 80, 85, 90]\n",
    "    \n",
    "    for cutoff in cutoff_range:\n",
    "        X, y = return_X_y_logistic(split_sample_combine(df, cutoff=cutoff, rand=0))\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "        X_train = standardize_X(X_train)\n",
    "        X_test = standardize_X(X_test)\n",
    "\n",
    "        X_train = X_train.values\n",
    "        y_train = y_train.values.ravel()\n",
    "\n",
    "        X_test = X_test.values\n",
    "        y_test = y_test.values.ravel()\n",
    "        \n",
    "        classifier = LogisticRegression()\n",
    "        y_predict_train = classifier.fit(X_train, y_train).predict(X_train)\n",
    "        probs_0and1_train = classifier.fit(X_train,y_train).predict_proba(X_train)\n",
    "        y_prob_P_train = probs_0and1_train[:,1]\n",
    "        \n",
    "        test_metrics = []\n",
    "        # calculate metrics for JUST train\n",
    "        test_metrics.append(metrics.roc_auc_score(y_train, y_prob_P_train))\n",
    "        test_metrics.append(accuracy_score(y_train, y_predict_train))\n",
    "        test_metrics.append(precision_score(y_train, y_predict_train))\n",
    "        test_metrics.append(recall_score(y_train, y_predict_train))\n",
    "        test_metrics.append(int(cutoff))\n",
    "        test_metrics.append(\"Test\")\n",
    "        \n",
    "        df_metrics.loc[cutoff] = test_metrics\n",
    "        df_metrics = df_metrics.reset_index(drop=True)\n",
    "        df_metrics[\"cutoff\"] = pd.to_numeric(df_metrics[\"cutoff\"])\n",
    "        \n",
    "    # plot metrics vs. popularity score cutoff\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.plot(df_metrics['cutoff'], df_metrics['auc'], color='b', lw=2, label='auc')\n",
    "    ax.plot(df_metrics['cutoff'], df_metrics['accuracy'], color='r', lw=2, label='accuracy')\n",
    "    ax.plot(df_metrics['cutoff'], df_metrics['precision'], color='g', lw=2, label='precision')\n",
    "    ax.plot(df_metrics['cutoff'], df_metrics['recall'], color='y', lw=2, label='recall')\n",
    "\n",
    "    ax.set_xlabel(\"Popularity Score Cutoff\", fontsize=20)\n",
    "    ax.set_ylabel(\"Area (auc) / Rate (others)\", fontsize=20)\n",
    "    ax.set_title(\"Metrics vs Popularity Score Cutoff Values - Training Dataset:\", fontsize=24)\n",
    "    ax.legend(fontsize=24)\n",
    "    plt.show()\n",
    "\n",
    "# plot a confusion matrix\n",
    "def plot_confusion_matrix(cm, ax, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Greens):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    font_size = 24\n",
    "    p = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.set_title(title,fontsize=font_size)\n",
    "    \n",
    "    tick_marks = np.arange(len(classes))\n",
    "    ax.set_xticks(tick_marks)\n",
    "    ax.set_xticklabels(classes, rotation=45, fontsize=16)\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_yticklabels(classes, fontsize=16)\n",
    "   \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if i == 1 and j == 1:\n",
    "            lbl = \"(True Positive)\"\n",
    "        elif i == 0 and j == 0:\n",
    "            lbl = \"(True Negative)\"\n",
    "        elif i == 1 and j == 0:\n",
    "            lbl = \"(False Negative)\"\n",
    "        elif i == 0 and j == 1:\n",
    "            lbl = \"(False Positive)\"\n",
    "        ax.text(j, i, \"{:0.2f} \\n{}\".format(cm[i, j], lbl),\n",
    "                 horizontalalignment=\"center\", size = font_size,\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    ax.set_ylabel('True',fontsize=font_size)\n",
    "    ax.set_xlabel('Predicted',fontsize=font_size)\n",
    "\n",
    "# plot confusion matrix for final Train dataset\n",
    "def plot_conf_matrix_Train():\n",
    "    fig = plt.figure(figsize=(12,11))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.grid(False)\n",
    "    class_names = [\"Not Popular\",\"Popular\"]\n",
    "    plot_confusion_matrix(conf_matrix_log80_train, ax, classes=class_names,normalize=True,\n",
    "                      title='Normalized Confusion Matrix, Train Dataset, threshold = 0.5')\n",
    "    plt.show()\n",
    "\n",
    "# plot confusion matrix for final Test dataset\n",
    "def plot_conf_matrix_Test():\n",
    "    fig = plt.figure(figsize=(12,11))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.grid(False)\n",
    "    class_names = [\"Not Popular\",\"Popular\"]\n",
    "    plot_confusion_matrix(conf_matrix_log80_test, ax, classes=class_names,normalize=True,\n",
    "                      title='Normalized Confusion Matrix, Test Dataset, threshold = 0.5')\n",
    "    plt.show()\n",
    "\n",
    "# plot final coefficients of logistic regression\n",
    "def plot_final_coeffs():\n",
    "    columns_bar = ['acousticness', 'danceability','duration_ms', 'energy', 'instrumentalness', \n",
    "                   'key', 'liveness','loudness', 'mode', 'speechiness', 'tempo', 'time_signature', \n",
    "                   'valence']\n",
    "    df_final_coefs = pd.DataFrame(data = final_coefs, columns = columns_bar)\n",
    "    df_final_coefs.plot(kind = 'bar', figsize=(10, 5), align='edge')\n",
    "    plt.show()\n",
    "\n",
    "def get_true_positives():\n",
    "    # Songs my test model predicted were popular that are actually popular (true positives)\n",
    "    print(df_test_results_log80[(df_test_results_log80['pop_predict_bin'] == 1) & (df_test_results_log80['pop_bin'] == 1)])\n",
    "\n",
    "def get_true_negatives():\n",
    "    # Songs my test model predicted were not popular that are not actually popular (true negatives)\n",
    "    print(df_test_results_log80[(df_test_results_log80['pop_predict_bin'] == 0) & (df_test_results_log80['pop_bin'] == 0)])\n",
    "\n",
    "def get_false_positives():\n",
    "    # Songs my testodel predicted were popular that are not actually popular (false positives)\n",
    "    print(df_test_results_log80[(df_test_results_log80['pop_predict_bin'] == 1) & (df_test_results_log80['pop_bin'] == 0)])\n",
    "    # calculate false positive rate\n",
    "    df_train_results_log80[(df_train_results_log80['pop_predict_bin'] == 1) & (df_train_results_log80['pop_bin'] == 0)].count() / df_train_results_log80[df_train_results_log80['pop_bin'] == 0].count()\n",
    "\n",
    "def get_false_negatives():\n",
    "    # Songs my test model predicted were not popular that are actually popular (false negatives)\n",
    "    print(df_test_results_log80[(df_test_results_log80['pop_predict_bin'] == 0) & (df_test_results_log80['pop_bin'] == 1)])\n",
    "\n",
    "def sanity_check_test():\n",
    "    # grab a record from the results dataframe\n",
    "    sanity_check_loc = df_test_results_log80[(df_test_results_log80['pop_predict_bin'] == 0) & (df_test_results_log80['pop_bin'] == 1)].iloc[0]\n",
    "    # set the probability that song has a popularity score >=80 = sanity_check_prob\n",
    "    sanity_check_prob = sanity_check_loc['pop_predict']\n",
    "\n",
    "    # print these to make sure they make sense\n",
    "    print(sanity_check_loc)\n",
    "    print(sanity_check_prob)\n",
    "\n",
    "    # this record coresponds to the 9th row of X_test within the logistic regression function (I know becuase I looked ;)\n",
    "    print(sanity_check[9, :])\n",
    "    \n",
    "    # grab the standardized variables from X_test\n",
    "    sanity_check_std_vars = sanity_check[9, :]\n",
    "    print(sanity_check_std_vars)\n",
    "\n",
    "    # multiply the standardized variables by the regression coefficients, sum them and add the intercept\n",
    "    mult_coefs_vars_add_intercept = sum(sanity_check_std_vars*final_coefs.reshape(13)) + final_intercept\n",
    "    print(mult_coefs_vars_add_intercept)\n",
    "\n",
    "    # since the log odds = P / 1-P, need to exponentiate this to get to the final predicted probability\n",
    "    exponentiated = np.exp(mult_coefs_vars_add_intercept)\n",
    "    print(exponentiated)\n",
    "\n",
    "    # finally, calculate P, the odds of popular (popularity score >= 80)\n",
    "    p = exponentiated / (1 + exponentiated)\n",
    "    print(p)\n",
    "\n",
    "    # does this equal what we think it should???\n",
    "    delta_ps = float(p - sanity_check_prob)\n",
    "    print(f\"Dela in p values is {delta_ps:.7f}, woo hoo!!!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load data\n",
    "    df = load_data()\n",
    "    # set nice view options for terminal viewing\n",
    "    set_view_options(max_cols=50, max_rows=50, max_colwidth=40, dis_width=250)\n",
    "\n",
    "    ''' All basic functions commented out so tons of plots don't pop up at once'''\n",
    "    # get basic info from dataset\n",
    "    #get_df_info(df)\n",
    "\n",
    "    # Take a look at the data with truncated columns\n",
    "    #describe_cols(df, 9)\n",
    "\n",
    "    # look at top correlations - look into multicollinearity\n",
    "    #get_top_abs_correlations(df, 10)\n",
    "    \n",
    "    ''' All these plots are commented out for now '''  \n",
    "    # scatter_plot(df, 'danceability', 'popularity')\n",
    "    # scatter_plot(df, 'duration_ms', 'popularity')\n",
    "    # scatter_plot(df, 'key', 'popularity')\n",
    "    # scatter_plot(df, 'acousticness', 'popularity')\n",
    "    \n",
    "    ''' Uncomment these to run any or all of the functions defined above \n",
    "        Many of these are plots, so commented out for now     ''' \n",
    "    #linear_regression_initial(df)\n",
    "    #plot_pop_dist(df)\n",
    "    #undersample_plot(df)\n",
    "    #get_stats(df)\n",
    "    #plot_univ_dists(df, 85)\n",
    "    #plot_violin(df, 55)\n",
    "    #plot_pairplot(df, 500, 55)\n",
    "    #plot_keys(df, 55)\n",
    "    #plot_heatmap(df)\n",
    "    #calc_ANOVA(df, 55)\n",
    "    #df_samples = random_under_sampler(df, 80)\n",
    "    #linear_regression_initial(df_samples)\n",
    "    #plot_hist(df_samples)\n",
    "    #search_artist_track_name(df, \"Chain\", \"Some\")\n",
    "    #df_cols = add_cols(df, 80)\n",
    "    #df_split = split_sample_combine(df, cutoff=65, rand=0)\n",
    "    #linear_regression_final(df_split, show_plots=False)\n",
    "    #linear_regression_sklearn(df_split, show_plots=True)\n",
    "    #basic_logistic_regression(df, cutoff=80, rand=0)\n",
    "    #logistic_regression_with_kfold(df, cutoff=80, rand=0)\n",
    "    #logistic_regression_with_kfold(df, cutoff=80, rand=0, sig_only=True)\n",
    "    #print_confusion_matrix(df, cutoff=80, rand=0)\n",
    "    \n",
    "    # Calculate and plot final logistic regression values\n",
    "    logistic_regression_final(df, plot_the_roc=False)\n",
    "    #plot_cutoffs_vs_metrics(df)\n",
    "    #plot_conf_matrix_Train()\n",
    "    #plot_conf_matrix_Test()\n",
    "    print(final_coefs)\n",
    "    #plot_final_coeffs()\n",
    "    #get_true_positives()\n",
    "    #get_true_negatives()\n",
    "    #get_false_positives()\n",
    "    #get_false_negatives()\n",
    "    sanity_check_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_info(df):\n",
    "    # take an initial look at our data\n",
    "    print(df.head())\n",
    "\n",
    "    # take a look at the columns in our data set\n",
    "    print(\"The columns are:\")\n",
    "    print(df.columns)\n",
    "\n",
    "    # look at data types for each\n",
    "    print(df.info())\n",
    "\n",
    "    # take a look at data types, and it looks like we have a pretty clean data set!\n",
    "    # However, I think the 0 popularity scores might throw the model(s) off a bit.\n",
    "    print(\"Do we have any nulls?\")\n",
    "    print(f\"Looks like we have {df.isnull().sum().sum()} nulls\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a look at the average popularity score\n",
    "    pop_mean = df['popularity'].mean()\n",
    "    print(pop_mean)\n",
    "\n",
    "    # Proportion of songs that are very popular\n",
    "    print(df[df['popularity'] >= 50 ]['popularity'].count() / df.shape[0])\n",
    "\n",
    "    # Unique artists and song counts by artist\n",
    "    print(df['artist_name'].unique().shape)\n",
    "    print(df['artist_name'].value_counts())\n",
    "\n",
    "# nice way to truncate the column names to display easier\n",
    "# can be used with various metrics\n",
    "def describe_cols(df, L=10):\n",
    "    '''Limit ENTIRE column width (including header)'''\n",
    "    # get the max col width\n",
    "    O = pd.get_option(\"display.max_colwidth\")\n",
    "    # set max col width to be L\n",
    "    pd.set_option(\"display.max_colwidth\", L)\n",
    "    print(df.rename(columns=lambda x: x[:L - 2] + '...' if len(x) > L else x).describe())\n",
    "    pd.set_option(\"display.max_colwidth\", O)\n",
    "\n",
    "# How many songs have a popularity score > 90??\n",
    "# Let's list these songs\n",
    "def most_popular_songs(df):\n",
    "    most_popular = df[df['popularity'] > 90]['popularity'].count()\n",
    "    print(df[df['popularity'] > 90][['artist_name', 'popularity']])\n",
    "\n",
    "# plot a scatter plot\n",
    "def scatter_plot(df, col_x, col_y):\n",
    "    plt.scatter(df[col_x], df[col_y], alpha=0.2)\n",
    "    plt.show()\n",
    "\n",
    "def plot_scatter_matrix(df, num_rows):\n",
    "    scatter_matrix(df[:num_rows], alpha=0.2, figsize=(6, 6), diagonal='kde')\n",
    "    plt.show()\n",
    "\n",
    "def calc_correlations(df, cutoff):\n",
    "    corr = df.corr()\n",
    "    print(corr[corr > cutoff])\n",
    "\n",
    "# get redundant pairs from DataFrame\n",
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal pairs of correlation matrix and all pairs we'll remove \n",
    "    (since pair each is doubled in corr matrix)'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            if df[cols[i]].dtype != 'object' and df[cols[j]].dtype != 'object':\n",
    "                # print(\"THIS IS NOT AN OBJECT, YO, so you CAN take a corr of it, smarty!\")\n",
    "                pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "# get top absolute correlations\n",
    "def get_top_abs_correlations(df, n=10):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    \n",
    "    print(\"The top absolute correlations are:\")\n",
    "    print(au_corr[0:n])\n",
    "    return au_corr[0:n]\n",
    "\n",
    "# initial linear regression function, and plots\n",
    "def linear_regression_initial(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    X_cols = ['acousticness', 'danceability', 'duration_ms', 'energy', \n",
    "          'instrumentalness', 'key', 'liveness', 'loudness', 'mode', \n",
    "          'speechiness', 'tempo', 'time_signature', 'valence']\n",
    "\n",
    "    y_col = ['popularity']\n",
    "\n",
    "    X = df[X_cols]\n",
    "    y = df[y_col]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "    X_train = sm.add_constant(X_train)\n",
    "\n",
    "    # Instantiate OLS model, fit, predict, get errors\n",
    "    model = sm.OLS(y_train, X_train)\n",
    "    results = model.fit()\n",
    "    fitted_vals = results.predict(X_train)\n",
    "    stu_resid = results.resid_pearson\n",
    "    residuals = results.resid\n",
    "    y_vals = pd.DataFrame({'residuals':residuals, 'fitted_vals':fitted_vals, \\\n",
    "                           'stu_resid': stu_resid})\n",
    "\n",
    "    # Print the results\n",
    "    print(results.summary())\n",
    "\n",
    "    # QQ Plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    plt.title(\"QQ Plot - Initial Linear Regression\")\n",
    "    fig = sm.qqplot(stu_resid, line='45', fit=True, ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "    # Residuals Plot\n",
    "    y_vals.plot(kind='scatter', x='fitted_vals', y='stu_resid')\n",
    "    plt.show()\n",
    "\n",
    "# print count of all zeros within the dataset\n",
    "def get_zeros(df):\n",
    "    print(df[df['popularity'] == 0 ]['popularity'].count())\n",
    "\n",
    "# plot polularity scores distribution\n",
    "def plot_pop_dist(df):\n",
    "    # set palette\n",
    "    sns.set_palette('muted')\n",
    "\n",
    "    # create initial figure\n",
    "    fig = plt.figure(figsize=(8,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    sns.distplot(df['popularity']/100, color='g', label=\"Popularity\").set_title(\"Distribution of Popularity Scores - Entire Data Set\")\n",
    "\n",
    "    # create x and y axis labels\n",
    "    plt.xlabel(\"Popularity\")\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# plot undersampling methodology\n",
    "def undersample_plot(df):\n",
    "    # set palette\n",
    "    sns.set_palette('muted')\n",
    "\n",
    "    # create initial figure\n",
    "    fig = plt.figure(figsize=(8,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    sns.distplot(df['popularity']/100, color='g', label=\"Popularity\").set_title(\"Illustration of Undersampling from Data Set\")\n",
    "    \n",
    "    # create line to shade to the right of\n",
    "    line = ax.get_lines()[-1]\n",
    "    x_line, y_line = line.get_data()\n",
    "    mask = x_line > 0.55\n",
    "    x_line, y_line = x_line[mask], y_line[mask]\n",
    "    ax.fill_between(x_line, y1=y_line, alpha=0.5, facecolor='red')\n",
    "\n",
    "    # get values for and plot first label\n",
    "    label_x = 0.5\n",
    "    label_y = 4\n",
    "    arrow_x = 0.6\n",
    "    arrow_y = 0.2\n",
    "\n",
    "    arrow_properties = dict(\n",
    "        facecolor=\"black\", width=2,\n",
    "        headwidth=4,connectionstyle='arc3,rad=0')\n",
    "\n",
    "    plt.annotate(\n",
    "        \"First, sample all songs in this range.\\n Sample size is n. Cutoff is 0.5.\", xy=(arrow_x, arrow_y),\n",
    "        xytext=(label_x, label_y),\n",
    "        bbox=dict(boxstyle='round,pad=0.5', fc='red', alpha=0.5),\n",
    "        arrowprops=arrow_properties)\n",
    "\n",
    "    # Get values for and plot second label\n",
    "    label_x = 0.1\n",
    "    label_y = 3\n",
    "    arrow_x = 0.2\n",
    "    arrow_y = 0.2\n",
    "\n",
    "    arrow_properties = dict(\n",
    "        facecolor=\"black\", width=2,\n",
    "        headwidth=4,connectionstyle='arc3,rad=0')\n",
    "\n",
    "    plt.annotate(\n",
    "        \"Next, randomly sample \\n n songs in this range\", xy=(arrow_x, arrow_y),\n",
    "        xytext=(label_x, label_y),\n",
    "        bbox=dict(boxstyle='round,pad=0.5', fc='g', alpha=0.5),\n",
    "        arrowprops=arrow_properties)\n",
    "\n",
    "    # plot final word box\n",
    "    plt.annotate(\n",
    "        \"Therefore, end up with a 50/50 \\n split of Popular / Not Popular\\n songs\", xy=(0.6, 2),\n",
    "        xytext=(0.62, 2),\n",
    "        bbox=dict(boxstyle='round,pad=0.5', fc='b', alpha=0.5))\n",
    "\n",
    "    # create x and y axis labels\n",
    "    plt.xlabel(\"Popularity\")\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# calculate and print more stats from the df\n",
    "def get_stats(df):\n",
    "    # print stats for various metrics\n",
    "    print(f\"There are {df.shape[0]} rows\")\n",
    "    print(f\"There are {df['track_id'].unique().shape} unique songs\")\n",
    "    print(f\"There are {df['artist_name'].unique().shape} unique artists\")\n",
    "    print(f\"There are {df['popularity'].unique().shape} popularity scores\")\n",
    "    print(f\"The mean popularity score is {df['popularity'].mean()}\")\n",
    "    print(f\"There are {df[df['popularity'] > 55]['popularity'].count()} songs with a popularity score > 55\")\n",
    "    print(f\"There are {df[df['popularity'] > 75]['popularity'].count()} songs with a popularity score > 75\")\n",
    "    print(f\"Only {(df[df['popularity'] > 80]['popularity'].count() / df.shape[0])*100:.2f} % of songs have a popularity score > 80\")\n",
    "\n",
    "# plot univariate dists for several independent variables\n",
    "def plot_univ_dists(df, cutoff):\n",
    "    popularity_cutoff = cutoff\n",
    "    print('Mean value for Danceability feature for Popular songs: {}'.format(df[df['popularity'] > popularity_cutoff]['danceability'].mean()))\n",
    "    print('Mean value for Danceability feature for Unpopular songs: {}'.format(df[df['popularity'] < popularity_cutoff]['danceability'].mean()))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8,5))\n",
    "    fig.suptitle('Histograms and Univariate Distributions of Important Features')\n",
    "    sns.distplot(df[df['popularity'] < popularity_cutoff]['danceability'])\n",
    "    sns.distplot(df[df['popularity'] > popularity_cutoff]['danceability'])\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8,5))\n",
    "    sns.distplot(df[df['popularity'] < popularity_cutoff]['valence'])\n",
    "    sns.distplot(df[df['popularity'] > popularity_cutoff]['valence'])\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8,5))\n",
    "    sns.distplot(df[df['popularity'] < popularity_cutoff]['acousticness'])\n",
    "    sns.distplot(df[df['popularity'] > popularity_cutoff]['acousticness'])\n",
    "    plt.show()\n",
    "\n",
    "# plot violin plot for several independent variables\n",
    "def plot_violin(df, cutoff):\n",
    "    df = df.copy()\n",
    "    \n",
    "    sns.set(style=\"whitegrid\")\n",
    "    df['pop_bin'] = np.where(df['popularity'] > cutoff, \"Popular\", \"Not_Popular\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, sharey=True, figsize=(12,4))\n",
    "    fig.suptitle('Distributions of Selected Features at Popularity Score Cutoff of 55')\n",
    "    \n",
    "    sns.violinplot(x=df['pop_bin'], y=df['danceability'], ax=ax[0])\n",
    "    sns.violinplot(x=df['pop_bin'], y=df['valence'], ax=ax[1])\n",
    "    sns.violinplot(x=df['pop_bin'], y=df['acousticness'], ax=ax[2])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, sharey=True, figsize=(12,4))\n",
    "    fig.suptitle('Distributions of Selected Features at Popularity Score Cutoff of 55')\n",
    "\n",
    "    sns.violinplot(x=df['pop_bin'], y=df['energy'], ax=ax[0])\n",
    "    sns.violinplot(x=df['pop_bin'], y=df['instrumentalness'], ax=ax[1])\n",
    "    sns.violinplot(x=df['pop_bin'], y=df['liveness'], ax=ax[2])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# plot pairplot for subsection of df rows and columns\n",
    "def plot_pairplot(df, rows, cutoff):\n",
    "    # not it looks MUCH better to run this function in jupyter\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['pop_bin'] = np.where(df['popularity'] > cutoff, \"Popular\", \"Not_Popular\")\n",
    "    \n",
    "    cols_for_pp = ['danceability', 'energy', 'instrumentalness',\n",
    "       'loudness','valence', 'popularity', 'pop_bin']\n",
    "\n",
    "    sns.pairplot(df.loc[:rows, cols_for_pp], hue='pop_bin', size=2)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# plot the key counts for popular and unpopular songs\n",
    "def plot_keys(df, cutoff):\n",
    "    df_popular = df[df['popularity'] > cutoff].copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, sharey=True, figsize=(8,5))\n",
    "    key_mapping = {0.0: 'C', 1.0: 'C♯,D♭', 2.0: 'D', 3.0: 'D♯,E♭', 4.0: 'E', 5.0: \n",
    "                  'F', 6.0: 'F♯,G♭', 7.0: 'G', 8.0: 'G♯,A♭', 9.0: 'A', 10.0: 'A♯,B♭', \n",
    "                  11.0: 'B'}\n",
    "    \n",
    "    df_popular['key_val'] = df_popular['key'].map(key_mapping)\n",
    "    sns.countplot(x='key_val', data=df_popular, order=df_popular['key_val'].value_counts().index, palette='muted')\n",
    "    plt.title(\"Key Totals for Popular Songs\")\n",
    "    plt.show()\n",
    "\n",
    "    df_unpopular = df[df['popularity'] < 55].copy()\n",
    "    fig, ax = plt.subplots(1, 1, sharey=True, figsize=(8,5))\n",
    "    df_unpopular['key_val'] = df_unpopular['key'].map(key_mapping)\n",
    "    sns.countplot(x='key_val', data=df_unpopular, order=df_unpopular['key_val'].value_counts().index, palette='muted')\n",
    "    plt.title(\"Key Totals for Unpopular Songs\")\n",
    "    plt.show()\n",
    "\n",
    "# plot a heatmap of the correlations between features as well as dependent variable\n",
    "def plot_heatmap(df):\n",
    "    # note this looks better in jupyter as well\n",
    "    plt.figure(figsize = (16,6))\n",
    "    sns.heatmap(df.corr(), cmap=\"coolwarm\", annot=True, )\n",
    "    plt.show()\n",
    "\n",
    "# check that deltas in means are significant for selected dependent variables\n",
    "def calc_ANOVA(df, cutoff):\n",
    "    df_popular = df[df['popularity'] > cutoff].copy()\n",
    "    df_unpopular = df[df['popularity'] < cutoff].copy()\n",
    "\n",
    "    print(\"Popular and Unpopular Danceability Means:\")  \n",
    "    print(df_popular['danceability'].mean())\n",
    "    print(df_unpopular['danceability'].mean())\n",
    "    f_val, p_val = stats.f_oneway(df_popular['danceability'], df_unpopular['danceability'])  \n",
    "    \n",
    "    print(\"Danceability One-way ANOVA P ={}\".format(p_val)) \n",
    "\n",
    "    print(\"Popular and Unpopular Loudness Means:\")  \n",
    "    print(df_popular['loudness'].mean())\n",
    "    print(df_unpopular['loudness'].mean())\n",
    "    f_val, p_val = stats.f_oneway(df_popular['loudness'], df_unpopular['loudness'])  \n",
    "    \n",
    "    print(\"Loudness One-way ANOVA P ={}\".format(p_val)) \n",
    "\n",
    "    print(df_popular['valence'].mean())\n",
    "    print(df_unpopular['valence'].mean())\n",
    "    f_val, p_val = stats.f_oneway(df_popular['valence'], df_unpopular['valence'])  \n",
    "    \n",
    "    print(\"Valence One-way ANOVA P ={}\".format(p_val))\n",
    "\n",
    "    print(df_popular['instrumentalness'].mean())\n",
    "    print(df_unpopular['instrumentalness'].mean())\n",
    "    f_val, p_val = stats.f_oneway(df_popular['instrumentalness'], df_unpopular['instrumentalness'])  \n",
    "    \n",
    "    print(\"Instrumentalness One-way ANOVA P ={}\".format(p_val))\n",
    "\n",
    "# randomly sample data below cutoff after choosing a cutoff so have a 50/50 split\n",
    "# of popular/unpopular target variable values.\n",
    "def random_under_sampler(df, cutoff):\n",
    "    df_original = df.copy()\n",
    "    df_original['pop_bin'] = np.where(df_original['popularity'] > cutoff, \"Popular\", \"Not_Popular\")\n",
    "\n",
    "    df_small = df_original[df_original['popularity'] > cutoff].copy()\n",
    "    df_samples_added = df_small.copy()\n",
    "    \n",
    "    total = df_small.shape[0] + 1\n",
    "\n",
    "    # loop through and add random unpopular rows to sampled df\n",
    "    while total <= df_small.shape[0]*2:\n",
    "\n",
    "        # pick a random from from the original dataframe\n",
    "        rand_row = random.randint(0,df_original.shape[0])\n",
    "        \n",
    "        if df_original.loc[rand_row, 'pop_bin'] == \"Not_Popular\":\n",
    "            df_samples_added.loc[total] = df_original.loc[rand_row, :]\n",
    "            total +=1\n",
    "\n",
    "    # print some stats on the undersampled df\n",
    "    print(\"Size checks for new df:\")\n",
    "    print(\"Shape of new undersampled df: {}\".format(df_samples_added.shape))\n",
    "    print(df_samples_added['pop_bin'].value_counts())\n",
    "    print(df_samples_added[df_samples_added['pop_bin'] == 'Popular']['danceability'].mean())\n",
    "    print(df_samples_added[df_samples_added['pop_bin'] == 'Not_Popular']['danceability'].mean())\n",
    "    print(df_samples_added[df_samples_added['pop_bin'] == 'Popular']['danceability'].count())\n",
    "    print(df_samples_added[df_samples_added['pop_bin'] == 'Not_Popular']['danceability'].count())\n",
    "    f_val, p_val = stats.f_oneway(df_samples_added[df_samples_added['pop_bin'] == 'Popular']['danceability'], df_samples_added[df_samples_added['pop_bin'] == 'Not_Popular']['danceability'])  \n",
    "  \n",
    "    print(\"One-way ANOVA P ={}\".format(p_val))\n",
    "\n",
    "    # return the df\n",
    "    return df_samples_added\n",
    "\n",
    "# plot histograms of metrics for popular and unpopular songs\n",
    "def plot_hist(sampled_df):\n",
    "    sampled_df[sampled_df['pop_bin'] == \"Popular\"].hist(figsize=(8, 8))  \n",
    "    plt.show()\n",
    "\n",
    "    sampled_df[sampled_df['pop_bin'] != \"Popular\"].hist(figsize=(8, 8))\n",
    "    plt.show()\n",
    "\n",
    "# return records that contain strings of artist and track names\n",
    "def search_artist_track_name(df, artist, track):\n",
    "    # this displays much better in jupyter\n",
    "    print(df[(df['artist_name'].str.contains(artist)) & (df['track_name'].str.contains(track))])\n",
    "\n",
    "    # use this if searching for A$AP rocky (or other artist with $ in the name)\n",
    "    # df[(df['artist_name'].str.contains(\"A\\$AP Rocky\"))]\n",
    "\n",
    "# add important columns to dataframe\n",
    "def add_cols(df, cutoff=55):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # add key_notes mapping key num vals to notes\n",
    "    key_mapping = {0.0: 'C', 1.0: 'C♯,D♭', 2.0: 'D', 3.0: 'D♯,E♭', \n",
    "                   4.0: 'E', 5.0: 'F', 6.0: 'F♯,G♭', 7.0: 'G', \n",
    "                   8.0: 'G♯,A♭', 9.0: 'A', 10.0: 'A♯,B♭', 11.0: 'B'}\n",
    "    df['key_notes'] = df['key'].map(key_mapping)\n",
    "    \n",
    "    # add columns relating to popularity\n",
    "    df['pop_frac'] = df['popularity'] / 100\n",
    "    df['pop_cat'] = np.where(df['popularity'] > cutoff, \"Popular\", \"Not_Popular\")\n",
    "    df['pop_bin'] = np.where(df['popularity'] > cutoff, 1, 0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# choose cutoff, sample popular data, randomly sample unpopular data, and combine the dfs\n",
    "def split_sample_combine(df, cutoff=55, col='popularity', rand=None):\n",
    "    # split out popular rows above the popularity cutoff\n",
    "    split_pop_df = df[df[col] > cutoff].copy()\n",
    "    \n",
    "    # get the leftover rows, the 'unpopular' songs\n",
    "    df_leftover = df[df[col] < cutoff].copy()\n",
    "    \n",
    "    # what % of the original data do we now have?\n",
    "    ratio = split_pop_df.shape[0] / df.shape[0]\n",
    "    \n",
    "    # what % of leftover rows do we need?\n",
    "    ratio_leftover = split_pop_df.shape[0] / df_leftover.shape[0]\n",
    "    \n",
    "    # get the exact # of unpopular rows needed, using a random sampler\n",
    "    unpop_df_leftover, unpop_df_to_add = train_test_split(df_leftover, \\\n",
    "                                                          test_size=ratio_leftover, \\\n",
    "                                                          random_state = rand)\n",
    "    \n",
    "    # combine the dataframes to get total rows = split_pop_df * 2\n",
    "    # ssc stands for \"split_sample_combine\"\n",
    "    ssc_df = split_pop_df.append(unpop_df_to_add).reset_index(drop=True)\n",
    "\n",
    "    # shuffle the df\n",
    "    ssc_df = ssc_df.sample(frac=1, random_state=rand).reset_index(drop=True)\n",
    "    \n",
    "    # add key_notes mapping key num vals to notes\n",
    "    key_mapping = {0.0: 'C', 1.0: 'C♯,D♭', 2.0: 'D', 3.0: 'D♯,E♭', \n",
    "                   4.0: 'E', 5.0: 'F', 6.0: 'F♯,G♭', 7.0: 'G', \n",
    "                   8.0: 'G♯,A♭', 9.0: 'A', 10.0: 'A♯,B♭', 11.0: 'B'}\n",
    "    ssc_df['key_notes'] = ssc_df['key'].map(key_mapping)\n",
    "    \n",
    "    # add columns relating to popularity\n",
    "    ssc_df['pop_frac'] = ssc_df['popularity'] / 100\n",
    "    ssc_df['pop_cat'] = np.where(ssc_df['popularity'] > cutoff, \"Popular\", \"Not_Popular\")\n",
    "    ssc_df['pop_bin'] = np.where(ssc_df['popularity'] > cutoff, 1, 0)\n",
    "    \n",
    "    return ssc_df\n",
    "\n",
    "# standardize data and return X and y dfs for linear regresssion\n",
    "def standardize_return_X_y(df, std=True, log=False):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # standardize some columns if std = True\n",
    "    if std == True:\n",
    "        for col in cols_to_standardize:\n",
    "            new_col_name = col + \"_std\"\n",
    "            df[new_col_name] = (df[col] - df[col].mean()) / df[col].std()\n",
    "\n",
    "        X_cols = ['acousticness', 'danceability', 'duration_ms_std', 'energy', \n",
    "                  'instrumentalness', 'key', 'liveness', 'loudness_std', 'mode', \n",
    "                  'speechiness', 'tempo_std', 'time_signature', 'valence']\n",
    "    else:\n",
    "        X_cols = ['acousticness', 'danceability', 'duration_ms', 'energy', \n",
    "                  'instrumentalness', 'key', 'liveness', 'loudness', 'mode', \n",
    "                  'speechiness', 'tempo', 'time_signature', 'valence']\n",
    "        \n",
    "    # if log = True, let's transform y to LOG\n",
    "    if log == True:\n",
    "        df['pop_log'] = df['popularity'] / 100\n",
    "        df['pop_log'] = [0.00000001 if x == 0 else x for x in df['pop_log']]\n",
    "        df['pop_log'] = [0.99999999 if x == 1 else x for x in df['pop_log']]\n",
    "        df['pop_log'] = np.log(df['pop_log'] / (1 - df['pop_log']))\n",
    "        y_col = ['pop_log']\n",
    "            \n",
    "    else:\n",
    "        y_col = ['popularity']\n",
    "\n",
    "    # split into X and y\n",
    "    X = df[X_cols]\n",
    "    y = df[y_col]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# final, clean, linear regression function\n",
    "def linear_regression_final(df, show_plots=True):\n",
    "    X, y = standardize_return_X_y(df, std=True, log=False)\n",
    "\n",
    "    # Add constant\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Instantiate OLS model, fit, predict, and get errors\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "    fitted_vals = results.predict(X)\n",
    "    stu_resid = results.resid_pearson\n",
    "    residuals = results.resid\n",
    "    y_vals = pd.DataFrame({'residuals':residuals, 'fitted_vals':fitted_vals, \\\n",
    "                           'stu_resid': stu_resid})\n",
    "\n",
    "    # Maybe do a line graph for this?\n",
    "    print(results.summary())\n",
    "    \n",
    "    ### Plot predicted values vs. actual/true\n",
    "    if show_plots == True:\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        plt.title(\"True vs. Predicted Popularity Values - Initial Linear Regression\")\n",
    "        plt.plot(y,alpha=0.2, label=\"True\")\n",
    "        plt.plot(fitted_vals,alpha=0.5, c='r', label=\"Predicted\")\n",
    "        plt.ylabel(\"Popularity\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # QQ Plot\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        fig = sm.qqplot(stu_resid, line='45', fit=True, ax=ax)\n",
    "        plt.show()\n",
    "  \n",
    "\n",
    "    # Residuals Plot\n",
    "        y_vals.plot(kind='scatter', y='fitted_vals', x='stu_resid')\n",
    "        plt.show()\n",
    "\n",
    "    return results\n",
    "\n",
    "# calculate root mean squared error\n",
    "def my_rmse(y_true, y_pred):\n",
    "    mse = ((y_true - y_pred)**2).mean()\n",
    "    return np.sqrt(mse)\n",
    "\n",
    "# create a linear regression using sklearn, in order to compare models, and \n",
    "# also incorporate train_test_split into this, and calculate and print RMSE\n",
    "def linear_regression_sklearn(df, show_plots=True):\n",
    "    X, y = standardize_return_X_y(df)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "    # Fit model using the training set\n",
    "    linear = LinearRegression()\n",
    "    linear.fit(X_train, y_train)\n",
    "\n",
    "    # Call predict to get the predicted values for training and test set\n",
    "    train_predicted = linear.predict(X_train)\n",
    "    test_predicted = linear.predict(X_test)\n",
    "\n",
    "    # Calculate RMSE for training and test set\n",
    "    print('RMSE for training set {}'.format(my_rmse(y_train.values, train_predicted)))\n",
    "    print('RMSE for test set {}'.format(my_rmse(y_test.values, test_predicted)))\n",
    "    print('The Coefficients are:')\n",
    "    print(linear.coef_)\n",
    "    print('The R^2 values is: {}'.format(linear.score(X_train, y_train)))\n",
    "\n",
    "    if show_plots == True:\n",
    "        plt.plot(y_train.reset_index(drop=True), alpha=0.2)\n",
    "        plt.plot(train_predicted, alpha=0.5, c='r')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(y_test.reset_index(drop=True), alpha=0.2)\n",
    "        plt.plot(test_predicted, alpha=0.5, c='r')\n",
    "        plt.show()\n",
    "\n",
    "# various data standardization and X/y split functions for logisitic reression\n",
    "# based on the columns you want to standardize and return\n",
    "def return_X_y_logistic(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # define columns to use for each\n",
    "    X_cols = ['acousticness', 'danceability', 'duration_ms', 'energy', \n",
    "              'instrumentalness', 'key', 'liveness', 'loudness', 'mode', \n",
    "              'speechiness', 'tempo', 'time_signature', 'valence']\n",
    "\n",
    "    # use 1's and 0's for logistic\n",
    "    y_col = ['pop_bin']\n",
    "\n",
    "    # split into X and y\n",
    "    X = df[X_cols]\n",
    "    y = df[y_col]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def return_X_y_logistic_more_cols(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # define columns to use for each\n",
    "    X_cols = ['artist_name','track_id','track_name','acousticness', 'danceability', 'duration_ms', 'energy', \n",
    "              'instrumentalness', 'key', 'liveness', 'loudness', 'mode', \n",
    "              'speechiness', 'tempo', 'time_signature', 'valence']\n",
    "\n",
    "    # use 1's and 0's for logistic\n",
    "    y_col = ['pop_bin']\n",
    "\n",
    "    # split into X and y\n",
    "    X = df[X_cols]\n",
    "    y = df[y_col]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def return_X_y_logistic_sig_only(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # define columns to use for each\n",
    "    X_cols = ['danceability','energy', \n",
    "              'instrumentalness', 'loudness']\n",
    "\n",
    "    # use 1's and 0's for logistic\n",
    "    y_col = ['pop_bin']\n",
    "\n",
    "    # split into X and y\n",
    "    X = df[X_cols]\n",
    "    y = df[y_col]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def standardize_X_sig_only(X):  \n",
    "    X = X.copy()\n",
    "    \n",
    "    cols = ['loudness']\n",
    "    # standardize only columns not between 0 and 1\n",
    "    for col in cols:\n",
    "        new_col_name = col + \"_std\"\n",
    "        X[new_col_name] = (X[col] - X[col].mean()) / X[col].std()\n",
    "        \n",
    "    X_cols = ['danceability','energy', \n",
    "              'instrumentalness', 'loudness_std']\n",
    "\n",
    "    # return the std columns in a dataframe\n",
    "    X = X[X_cols]\n",
    "    \n",
    "    return X\n",
    "\n",
    "def standardize_X(X):  \n",
    "    X = X.copy()\n",
    "    \n",
    "    # standardize only columns not between 0 and 1\n",
    "    for col in cols_to_standardize:\n",
    "        new_col_name = col + \"_std\"\n",
    "        X[new_col_name] = (X[col] - X[col].mean()) / X[col].std()\n",
    "        \n",
    "    X_cols = ['acousticness', 'danceability', 'duration_ms_std', 'energy', \n",
    "                  'instrumentalness', 'key', 'liveness', 'loudness_std', 'mode', \n",
    "                  'speechiness', 'tempo_std', 'time_signature', 'valence']\n",
    "\n",
    "    # return the std columns in a dataframe\n",
    "    X = X[X_cols]\n",
    "    \n",
    "    return X\n",
    "\n",
    "def standardize_X_train_test(X_train, X_test):  \n",
    "    X_train = X_train.copy()\n",
    "    X_test = X_test.copy() \n",
    "    \n",
    "    # standardize only columns not between 0 and 1\n",
    "    for col in cols_to_standardize:\n",
    "        new_col_name = col + \"_std\"\n",
    "        X_train[new_col_name] = (X_train[col] - X_train[col].mean()) / X_train[col].std()\n",
    "        X_test[new_col_name] = (X_test[col] - X_test[col].mean()) / X_test[col].std()\n",
    "    \n",
    "    X_cols = ['acousticness', 'danceability', 'duration_ms_std', 'energy', \n",
    "                  'instrumentalness', 'key', 'liveness', 'loudness_std', 'mode', \n",
    "                  'speechiness', 'tempo_std', 'time_signature', 'valence']\n",
    "\n",
    "    # return the std columns in a dataframe\n",
    "    X_train_std = X_train[X_cols]\n",
    "    X_test_std = X_test[X_cols]\n",
    "    \n",
    "    return X_train_std, X_test_std\n",
    "\n",
    "# Create a basic logistic regression\n",
    "def basic_logistic_regression(df, cutoff=55, rand=0, sig_only=False):\n",
    "    df = df.copy()\n",
    "\n",
    "    if sig_only == True:\n",
    "        X, y = return_X_y_logistic_sig_only(split_sample_combine(df, cutoff=cutoff, rand=rand))\n",
    "        X = standardize_X_sig_only(X)\n",
    "\n",
    "    else:\n",
    "        X, y = return_X_y_logistic(split_sample_combine(df, cutoff=80, rand=rand))\n",
    "        X = standardize_X(X)\n",
    "\n",
    "    X_const = add_constant(X, prepend=True)\n",
    "\n",
    "    logit_model = Logit(y, X_const).fit()\n",
    "    \n",
    "    print(logit_model.summary())\n",
    "\n",
    "    return logit_model\n",
    "\n",
    "def logistic_regression_with_kfold(df, cutoff=55, rand=0, sig_only=False):\n",
    "    df = df.copy()\n",
    "    \n",
    "    if sig_only == True:\n",
    "        X, y = return_X_y_logistic_sig_only(split_sample_combine(df, cutoff=cutoff, rand=rand))\n",
    "        X = standardize_X_sig_only(X)\n",
    "\n",
    "    else:\n",
    "        X, y = return_X_y_logistic(split_sample_combine(df, cutoff=cutoff, rand=rand))\n",
    "        X = standardize_X(X)\n",
    "\n",
    "    X = X.values\n",
    "    y = y.values.ravel()\n",
    "\n",
    "    classifier = LogisticRegression()\n",
    "\n",
    "    # before kFold\n",
    "    y_predict = classifier.fit(X, y).predict(X)\n",
    "    y_true = y\n",
    "    accuracy_score(y_true, y_predict)\n",
    "    print(f\"accuracy: {accuracy_score(y_true, y_predict)}\")\n",
    "    print(f\"precision: {precision_score(y_true, y_predict)}\")\n",
    "    print(f\"recall: {recall_score(y_true, y_predict)}\")\n",
    "    print(f\"The coefs are: {classifier.fit(X,y).coef_}\")\n",
    "\n",
    "    # with kfold\n",
    "    kfold = KFold(len(y))\n",
    "\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for train_index, test_index in kfold:\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X[train_index], y[train_index])\n",
    "\n",
    "        y_predict = model.predict(X[test_index])\n",
    "        y_true = y[test_index]\n",
    "\n",
    "        accuracies.append(accuracy_score(y_true, y_predict))\n",
    "        precisions.append(precision_score(y_true, y_predict))\n",
    "        recalls.append(recall_score(y_true, y_predict))\n",
    "\n",
    "    print(f\"accuracy: {np.average(accuracies)}\")\n",
    "    print(f\"precision: {np.average(precisions)}\")\n",
    "    print(f\"recall: {np.average(recalls)}\")\n",
    "\n",
    "# this is the code for the final logistic regression I chose, after running all the above\n",
    "# logistic regression models and k-fold cross-val analysis\n",
    "def logistic_regression_final(df, plot_the_roc=True):\n",
    "    df = df.copy()\n",
    "    cutoff = 80\n",
    "    \n",
    "    X, y = return_X_y_logistic_more_cols(split_sample_combine(df, cutoff=cutoff, rand=2))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=2)\n",
    "\n",
    "    global df_train_results_log80 \n",
    "    global df_test_results_log80\n",
    "    df_train_results_log80 = X_train.join(y_train)\n",
    "    df_test_results_log80 = X_test.join(y_test)\n",
    "\n",
    "    # standardize X_train and X_test\n",
    "    X_train = standardize_X(X_train)\n",
    "    X_test = standardize_X(X_test)\n",
    "\n",
    "    X_train = X_train.values\n",
    "    y_train = y_train.values.ravel()\n",
    "\n",
    "    X_test = X_test.values\n",
    "    y_test = y_test.values.ravel()\n",
    "\n",
    "    global sanity_check\n",
    "    sanity_check = X_test\n",
    "\n",
    "    ## Run logistic regression on all the data\n",
    "    classifier = LogisticRegression()\n",
    "    # note using .predict_proba() below, which is the probability of each class\n",
    "    \n",
    "    #predict values for X_train\n",
    "    y_predict_train = classifier.fit(X_train,y_train).predict(X_train)\n",
    "    probs_0and1_train = classifier.fit(X_train,y_train).predict_proba(X_train)\n",
    "    y_prob_P_train = probs_0and1_train[:,1]\n",
    "\n",
    "    # predict values for X_test\n",
    "    y_predict_test = classifier.fit(X_train,y_train).predict(X_test)\n",
    "    probs_0and1_test = classifier.fit(X_train,y_train).predict_proba(X_test) # yes!\n",
    "    y_prob_P_test = probs_0and1_test[:,1]\n",
    "\n",
    "    # calculate metrics needed to use for ROC curve below\n",
    "    fpr_train, tpr_train, thresholds_train = metrics.roc_curve(y_train, y_prob_P_train, pos_label=1)\n",
    "    auc_train = metrics.roc_auc_score(y_train, y_prob_P_train) # note we are scoring on our training data!\n",
    "\n",
    "    fpr_test, tpr_test, thresholds_test = metrics.roc_curve(y_test, y_prob_P_test, pos_label=1)\n",
    "    auc_test = metrics.roc_auc_score(y_test, y_prob_P_test) # note we are scoring on our training data!\n",
    "\n",
    "    # print some metrics\n",
    "    print(\"Train accuracy: {:.2f}\".format(accuracy_score(y_train, y_predict_train)))\n",
    "    print(\"Test accuracy: {:.2f}\".format(accuracy_score(y_test, y_predict_test)))\n",
    "\n",
    "    print(\"Train recall: {:.2f}\".format(recall_score(y_train, y_predict_train)))\n",
    "    print(\"Test recall: {:.2f}\".format(recall_score(y_test, y_predict_test)))\n",
    "\n",
    "    print(\"Train precision: {:.2f}\".format(precision_score(y_train, y_predict_train)))\n",
    "    print(\"Test precision: {:.2f}\".format(precision_score(y_test, y_predict_test)))\n",
    "\n",
    "    print(\"Train auc: {:.2f}\".format(auc_train))\n",
    "    print(\"Test auc: {:.2f}\".format(auc_test))\n",
    "\n",
    "    global conf_matrix_log80_train\n",
    "    global conf_matrix_log80_test\n",
    "    conf_matrix_log80_train = confusion_matrix(y_train, y_predict_train)\n",
    "    conf_matrix_log80_test = confusion_matrix(y_test, y_predict_test)\n",
    "\n",
    "    global final_coefs\n",
    "    global final_intercept\n",
    "    final_coefs = classifier.fit(X_train,y_train).coef_\n",
    "    final_intercept = classifier.fit(X_train,y_train).intercept_\n",
    "\n",
    "    # Back of the envelope calcs to make sure metrics above are correct\n",
    "    df_train_results_log80 = df_train_results_log80.reset_index(drop=True)\n",
    "    df_train_results_log80['pop_predict'] = y_prob_P_train\n",
    "\n",
    "    df_test_results_log80 = df_test_results_log80.reset_index(drop=True)\n",
    "    df_test_results_log80['pop_predict'] = y_prob_P_test\n",
    "\n",
    "    df_train_results_log80['pop_predict_bin'] = np.where(df_train_results_log80['pop_predict'] >= 0.5, 1, 0)\n",
    "    df_test_results_log80['pop_predict_bin'] = np.where(df_test_results_log80['pop_predict'] >= 0.5, 1, 0)\n",
    "    \n",
    "    print(\"Back of the envelope calc for Train Recall\")\n",
    "    print(sum((df_train_results_log80['pop_predict_bin'].values * df_train_results_log80['pop_bin'].values))/ df_train_results_log80['pop_bin'].sum())\n",
    "\n",
    "    if plot_the_roc == True:\n",
    "        # Plot the ROC\n",
    "        fig = plt.figure(figsize=(10,8))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k',\n",
    "                label='Luck')\n",
    "        ax.plot(fpr_train, tpr_train, color='b', lw=2, label='Model_Train')\n",
    "        ax.plot(fpr_test, tpr_test, color='r', lw=2, label='Model_Test')\n",
    "        ax.set_xlabel(\"False Positive Rate\", fontsize=20)\n",
    "        ax.set_ylabel(\"True Positive Rate\", fontsize=20)\n",
    "        ax.set_title(\"ROC curve - Cutoff: \" + str(cutoff), fontsize=24)\n",
    "        ax.text(0.05, 0.95, \" \".join([\"AUC_train:\",str(auc_train.round(3))]), fontsize=20)\n",
    "        ax.text(0.32, 0.7, \" \".join([\"AUC_test:\",str(auc_test.round(3))]), fontsize=20)\n",
    "        ax.legend(fontsize=24)\n",
    "        plt.show()\n",
    "\n",
    "# print out confusion matrix\n",
    "def print_confusion_matrix(df, cutoff=55, rand=0):\n",
    "    df = df.copy()\n",
    "\n",
    "    X, y = return_X_y_logistic(split_sample_combine(df, cutoff=80, rand=rand))\n",
    "    X = standardize_X(X)\n",
    "\n",
    "    X = X.values\n",
    "    y = y.values.ravel()\n",
    "\n",
    "    ## Run logistic regression on all the data\n",
    "    classifier = LogisticRegression()\n",
    "    # note using .predict() below, which uses default 0.5 for a binary classifier\n",
    "    y_pred = classifier.fit(X,y).predict(X) # agh! this uses 0.5 threshold for binary classifier\n",
    "    y_true = y\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "    print(\"| TN | FP |\\n| FN | TP |\\n\")\n",
    "    print(cnf_matrix)\n",
    "    print(f\"The accurracy is {accuracy_score(y_true, y_pred)}\")\n",
    "    print(f\"The accurracy (check) is {(cnf_matrix[1][1]+ cnf_matrix[0][0])/np.sum(cnf_matrix)}\")\n",
    "\n",
    "# plot popularity score cutoffs vs. logistic regression metrics\n",
    "def plot_cutoffs_vs_metrics(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    df_cols = ['auc', 'accuracy', 'precision', 'recall', 'cutoff', 'type']\n",
    "    df_metrics = pd.DataFrame(columns = df_cols)\n",
    "    cutoff_range = [45, 55, 60, 65, 70, 75, 80, 85, 90]\n",
    "    \n",
    "    for cutoff in cutoff_range:\n",
    "        X, y = return_X_y_logistic(split_sample_combine(df, cutoff=cutoff, rand=0))\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "        X_train = standardize_X(X_train)\n",
    "        X_test = standardize_X(X_test)\n",
    "\n",
    "        X_train = X_train.values\n",
    "        y_train = y_train.values.ravel()\n",
    "\n",
    "        X_test = X_test.values\n",
    "        y_test = y_test.values.ravel()\n",
    "        \n",
    "        classifier = LogisticRegression()\n",
    "        y_predict_train = classifier.fit(X_train, y_train).predict(X_train)\n",
    "        probs_0and1_train = classifier.fit(X_train,y_train).predict_proba(X_train)\n",
    "        y_prob_P_train = probs_0and1_train[:,1]\n",
    "        \n",
    "        test_metrics = []\n",
    "        # calculate metrics for JUST train\n",
    "        test_metrics.append(metrics.roc_auc_score(y_train, y_prob_P_train))\n",
    "        test_metrics.append(accuracy_score(y_train, y_predict_train))\n",
    "        test_metrics.append(precision_score(y_train, y_predict_train))\n",
    "        test_metrics.append(recall_score(y_train, y_predict_train))\n",
    "        test_metrics.append(int(cutoff))\n",
    "        test_metrics.append(\"Test\")\n",
    "        \n",
    "        df_metrics.loc[cutoff] = test_metrics\n",
    "        df_metrics = df_metrics.reset_index(drop=True)\n",
    "        df_metrics[\"cutoff\"] = pd.to_numeric(df_metrics[\"cutoff\"])\n",
    "        \n",
    "    # plot metrics vs. popularity score cutoff\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.plot(df_metrics['cutoff'], df_metrics['auc'], color='b', lw=2, label='auc')\n",
    "    ax.plot(df_metrics['cutoff'], df_metrics['accuracy'], color='r', lw=2, label='accuracy')\n",
    "    ax.plot(df_metrics['cutoff'], df_metrics['precision'], color='g', lw=2, label='precision')\n",
    "    ax.plot(df_metrics['cutoff'], df_metrics['recall'], color='y', lw=2, label='recall')\n",
    "\n",
    "    ax.set_xlabel(\"Popularity Score Cutoff\", fontsize=20)\n",
    "    ax.set_ylabel(\"Area (auc) / Rate (others)\", fontsize=20)\n",
    "    ax.set_title(\"Metrics vs Popularity Score Cutoff Values - Training Dataset:\", fontsize=24)\n",
    "    ax.legend(fontsize=24)\n",
    "    plt.show()\n",
    "\n",
    "# plot a confusion matrix\n",
    "def plot_confusion_matrix(cm, ax, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Greens):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    font_size = 24\n",
    "    p = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.set_title(title,fontsize=font_size)\n",
    "    \n",
    "    tick_marks = np.arange(len(classes))\n",
    "    ax.set_xticks(tick_marks)\n",
    "    ax.set_xticklabels(classes, rotation=45, fontsize=16)\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_yticklabels(classes, fontsize=16)\n",
    "   \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if i == 1 and j == 1:\n",
    "            lbl = \"(True Positive)\"\n",
    "        elif i == 0 and j == 0:\n",
    "            lbl = \"(True Negative)\"\n",
    "        elif i == 1 and j == 0:\n",
    "            lbl = \"(False Negative)\"\n",
    "        elif i == 0 and j == 1:\n",
    "            lbl = \"(False Positive)\"\n",
    "        ax.text(j, i, \"{:0.2f} \\n{}\".format(cm[i, j], lbl),\n",
    "                 horizontalalignment=\"center\", size = font_size,\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    ax.set_ylabel('True',fontsize=font_size)\n",
    "    ax.set_xlabel('Predicted',fontsize=font_size)\n",
    "\n",
    "# plot confusion matrix for final Train dataset\n",
    "def plot_conf_matrix_Train():\n",
    "    fig = plt.figure(figsize=(12,11))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.grid(False)\n",
    "    class_names = [\"Not Popular\",\"Popular\"]\n",
    "    plot_confusion_matrix(conf_matrix_log80_train, ax, classes=class_names,normalize=True,\n",
    "                      title='Normalized Confusion Matrix, Train Dataset, threshold = 0.5')\n",
    "    plt.show()\n",
    "\n",
    "# plot confusion matrix for final Test dataset\n",
    "def plot_conf_matrix_Test():\n",
    "    fig = plt.figure(figsize=(12,11))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.grid(False)\n",
    "    class_names = [\"Not Popular\",\"Popular\"]\n",
    "    plot_confusion_matrix(conf_matrix_log80_test, ax, classes=class_names,normalize=True,\n",
    "                      title='Normalized Confusion Matrix, Test Dataset, threshold = 0.5')\n",
    "    plt.show()\n",
    "\n",
    "# plot final coefficients of logistic regression\n",
    "def plot_final_coeffs():\n",
    "    columns_bar = ['acousticness', 'danceability','duration_ms', 'energy', 'instrumentalness', \n",
    "                   'key', 'liveness','loudness', 'mode', 'speechiness', 'tempo', 'time_signature', \n",
    "                   'valence']\n",
    "    df_final_coefs = pd.DataFrame(data = final_coefs, columns = columns_bar)\n",
    "    df_final_coefs.plot(kind = 'bar', figsize=(10, 5), align='edge')\n",
    "    plt.show()\n",
    "\n",
    "def get_true_positives():\n",
    "    # Songs my test model predicted were popular that are actually popular (true positives)\n",
    "    print(df_test_results_log80[(df_test_results_log80['pop_predict_bin'] == 1) & (df_test_results_log80['pop_bin'] == 1)])\n",
    "\n",
    "def get_true_negatives():\n",
    "    # Songs my test model predicted were not popular that are not actually popular (true negatives)\n",
    "    print(df_test_results_log80[(df_test_results_log80['pop_predict_bin'] == 0) & (df_test_results_log80['pop_bin'] == 0)])\n",
    "\n",
    "def get_false_positives():\n",
    "    # Songs my testodel predicted were popular that are not actually popular (false positives)\n",
    "    print(df_test_results_log80[(df_test_results_log80['pop_predict_bin'] == 1) & (df_test_results_log80['pop_bin'] == 0)])\n",
    "    # calculate false positive rate\n",
    "    df_train_results_log80[(df_train_results_log80['pop_predict_bin'] == 1) & (df_train_results_log80['pop_bin'] == 0)].count() / df_train_results_log80[df_train_results_log80['pop_bin'] == 0].count()\n",
    "\n",
    "def get_false_negatives():\n",
    "    # Songs my test model predicted were not popular that are actually popular (false negatives)\n",
    "    print(df_test_results_log80[(df_test_results_log80['pop_predict_bin'] == 0) & (df_test_results_log80['pop_bin'] == 1)])\n",
    "\n",
    "def sanity_check_test():\n",
    "    # grab a record from the results dataframe\n",
    "    sanity_check_loc = df_test_results_log80[(df_test_results_log80['pop_predict_bin'] == 0) & (df_test_results_log80['pop_bin'] == 1)].iloc[0]\n",
    "    # set the probability that song has a popularity score >=80 = sanity_check_prob\n",
    "    sanity_check_prob = sanity_check_loc['pop_predict']\n",
    "\n",
    "    # print these to make sure they make sense\n",
    "    print(sanity_check_loc)\n",
    "    print(sanity_check_prob)\n",
    "\n",
    "    # this record coresponds to the 9th row of X_test within the logistic regression function (I know becuase I looked ;)\n",
    "    print(sanity_check[9, :])\n",
    "    \n",
    "    # grab the standardized variables from X_test\n",
    "    sanity_check_std_vars = sanity_check[9, :]\n",
    "    print(sanity_check_std_vars)\n",
    "\n",
    "    # multiply the standardized variables by the regression coefficients, sum them and add the intercept\n",
    "    mult_coefs_vars_add_intercept = sum(sanity_check_std_vars*final_coefs.reshape(13)) + final_intercept\n",
    "    print(mult_coefs_vars_add_intercept)\n",
    "\n",
    "    # since the log odds = P / 1-P, need to exponentiate this to get to the final predicted probability\n",
    "    exponentiated = np.exp(mult_coefs_vars_add_intercept)\n",
    "    print(exponentiated)\n",
    "\n",
    "    # finally, calculate P, the odds of popular (popularity score >= 80)\n",
    "    p = exponentiated / (1 + exponentiated)\n",
    "    print(p)\n",
    "\n",
    "    # does this equal what we think it should???\n",
    "    delta_ps = float(p - sanity_check_prob)\n",
    "    print(f\"Dela in p values is {delta_ps:.7f}, woo hoo!!!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load data\n",
    "    df = load_data()\n",
    "    # set nice view options for terminal viewing\n",
    "    set_view_options(max_cols=50, max_rows=50, max_colwidth=40, dis_width=250)\n",
    "\n",
    "    ''' All basic functions commented out so tons of plots don't pop up at once'''\n",
    "    # get basic info from dataset\n",
    "    #get_df_info(df)\n",
    "\n",
    "    # Take a look at the data with truncated columns\n",
    "    #describe_cols(df, 9)\n",
    "\n",
    "    # look at top correlations - look into multicollinearity\n",
    "    #get_top_abs_correlations(df, 10)\n",
    "    \n",
    "    ''' All these plots are commented out for now '''  \n",
    "    # scatter_plot(df, 'danceability', 'popularity')\n",
    "    # scatter_plot(df, 'duration_ms', 'popularity')\n",
    "    # scatter_plot(df, 'key', 'popularity')\n",
    "    # scatter_plot(df, 'acousticness', 'popularity')\n",
    "    \n",
    "    ''' Uncomment these to run any or all of the functions defined above \n",
    "        Many of these are plots, so commented out for now     ''' \n",
    "    #linear_regression_initial(df)\n",
    "    #plot_pop_dist(df)\n",
    "    #undersample_plot(df)\n",
    "    #get_stats(df)\n",
    "    #plot_univ_dists(df, 85)\n",
    "    #plot_violin(df, 55)\n",
    "    #plot_pairplot(df, 500, 55)\n",
    "    #plot_keys(df, 55)\n",
    "    #plot_heatmap(df)\n",
    "    #calc_ANOVA(df, 55)\n",
    "    #df_samples = random_under_sampler(df, 80)\n",
    "    #linear_regression_initial(df_samples)\n",
    "    #plot_hist(df_samples)\n",
    "    #search_artist_track_name(df, \"Chain\", \"Some\")\n",
    "    #df_cols = add_cols(df, 80)\n",
    "    #df_split = split_sample_combine(df, cutoff=65, rand=0)\n",
    "    #linear_regression_final(df_split, show_plots=False)\n",
    "    #linear_regression_sklearn(df_split, show_plots=True)\n",
    "    #basic_logistic_regression(df, cutoff=80, rand=0)\n",
    "    #logistic_regression_with_kfold(df, cutoff=80, rand=0)\n",
    "    #logistic_regression_with_kfold(df, cutoff=80, rand=0, sig_only=True)\n",
    "    #print_confusion_matrix(df, cutoff=80, rand=0)\n",
    "    \n",
    "    # Calculate and plot final logistic regression values\n",
    "    logistic_regression_final(df, plot_the_roc=False)\n",
    "    #plot_cutoffs_vs_metrics(df)\n",
    "    #plot_conf_matrix_Train()\n",
    "    #plot_conf_matrix_Test()\n",
    "    print(final_coefs)\n",
    "    #plot_final_coeffs()\n",
    "    #get_true_positives()\n",
    "    #get_true_negatives()\n",
    "    #get_false_positives()\n",
    "    #get_false_negatives()\n",
    "    sanity_check_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
